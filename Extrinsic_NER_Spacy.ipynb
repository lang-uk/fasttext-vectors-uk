{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lang-uk/fasttext-vectors-uk/blob/main/Extrinsic_NER_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZp05dvGroDf",
        "outputId": "81b891d6-db3e-43ba-d9d6-51ece9181daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M57oJbWtRFO",
        "outputId": "987c3e87-6f43-4785-a17c-bb5477b6efcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 13.7 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 89.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 86.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 92.9 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 87.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3D0QQc7sCdp",
        "outputId": "abe12b3d-3312-4359-8c9e-ed786962d3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4045 documents): /tmp/test.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8762 documents): /tmp/train.spacy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy convert /gdrive/MyDrive/NER_UK/fixed-split/test.iob /tmp\n",
        "!python -m spacy convert /gdrive/MyDrive/NER_UK/fixed-split/train.iob /tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHr-5jootsPw",
        "outputId": "26ff9021-c266-4276-831a-8711157391d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.2.1                         \n",
            "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
            "Platform         Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version   3.7.12                        \n",
            "Pipelines                                      \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ZkhRLjftzC",
        "outputId": "9e62ee4c-4d20-472b-c619-0e12dd3881ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.2 MB/s eta 0:16:13tcmalloc: large alloc 1147494400 bytes == 0x561148bac000 @  0x7f343135e615 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x56111052cd00 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611104bb039 0x5611104fe409 0x5611104b9c52 0x56111052cc25 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110528915 0x5611104baafa 0x561110528c0d 0x5611105279ee\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 122.2 MB/s eta 0:00:09tcmalloc: large alloc 1434370048 bytes == 0x56118d202000 @  0x7f343135e615 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x56111052cd00 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611104bb039 0x5611104fe409 0x5611104b9c52 0x56111052cc25 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110528915 0x5611104baafa 0x561110528c0d 0x5611105279ee\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.2 MB/s eta 0:10:04tcmalloc: large alloc 1792966656 bytes == 0x561112034000 @  0x7f343135e615 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x56111052cd00 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611104bb039 0x5611104fe409 0x5611104b9c52 0x56111052cc25 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110528915 0x5611104baafa 0x561110528c0d 0x5611105279ee\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:05:01tcmalloc: large alloc 2241208320 bytes == 0x56117ce1c000 @  0x7f343135e615 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x56111052cd00 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611105abc66 0x561110528daf 0x5611104bb039 0x5611104fe409 0x5611104b9c52 0x56111052cc25 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110528915 0x5611104baafa 0x561110528c0d 0x5611105279ee\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x56120277e000 @  0x7f343135d1e7 0x5611104ec067 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611104baafa 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee\n",
            "tcmalloc: large alloc 2551685120 bytes == 0x5612f0708000 @  0x7f343135e615 0x5611104b64cc 0x56111059647a 0x5611104b92ed 0x5611105aae1d 0x56111052ce99 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110528c0d 0x5611104baafa 0x561110528c0d 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104babda 0x561110529737 0x5611105279ee 0x5611104bb271\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 5.2 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAM_cDixaHsX",
        "outputId": "ba367229-cd2d-4104-d8f1-2da7357fadc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.22 MiB | 24.55 MiB/s, done.\n",
            "Resolving deltas: 100% (2417/2417), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcLwxtkYaSq2",
        "outputId": "2c4c2c34-f93c-49e4-e977-da4a4a555690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.0-py2.py3-none-any.whl (210 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3120880 sha256=523a818128828258b85c5c9e32cce9a3ec82b9daaf11ffa331f65de599dac911\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pdp3jb0q/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!cd fastText && pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E_GUUhhaiYB"
      },
      "outputs": [],
      "source": [
        "from fasttext import load_model\n",
        "import argparse\n",
        "import errno\n",
        "\n",
        "def convert_bin_to_vec(model, output):\n",
        "    f = load_model(model)\n",
        "    words = f.get_words()\n",
        "    with open(output, \"w\") as fp_out:\n",
        "        fp_out.write(str(len(words)) + \" \" + str(f.get_dimension()) + \"\\n\")\n",
        "        for w in words:\n",
        "            v = f.get_word_vector(w)\n",
        "            vstr = \"\"\n",
        "            for vi in v:\n",
        "                vstr += \" \" + str(vi)\n",
        "            try:\n",
        "                fp_out.write(w + vstr + \"\\n\")\n",
        "            except IOError as e:\n",
        "                if e.errno == errno.EPIPE:\n",
        "                    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgk2rilHlaE-",
        "outputId": "feafc818-34a6-4399-fa86-d71f3e6a4866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-3.neg_sampling-15 as their already exists\n",
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-4.neg_sampling-10 as their already exists\n",
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-4.neg_sampling-10 as their already exists\n",
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-4.neg_sampling-15 as their already exists\n",
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-3.neg_sampling-10 as their already exists\n",
            "Skipping ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-4.neg_sampling-15 as their already exists\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 16:49:29,785] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55889ca86000 @  0x7fdc3ffc5001 0x7fdc3dac354f 0x7fdc3db13b58 0x7fdc3db17b17 0x7fdc3dbb6203 0x558893952544 0x558893952240 0x5588939c6627 0x5588939c0ced 0x558893953bda 0x5588939c2737 0x5588939c0ced 0x558893953bda 0x5588939c2737 0x5588939c09ee 0x558893892e2b 0x5588939c2fe4 0x5588939c0ced 0x558893892e2b 0x5588939c2fe4 0x5588939c09ee 0x55889395448c 0x558893954698 0x5588939c2fe4 0x558893953afa 0x5588939c1c0d 0x5588939c0ced 0x558893953bda 0x5588939c1c0d 0x5588939c0ced 0x558893953bda\n",
            "2665029it [03:00, 14760.29it/s]\n",
            "[2022-01-05 16:52:31,893] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 16:54:11,160] [INFO] Set up nlp object from config\n",
            "[2022-01-05 16:54:11,170] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 16:54:11,174] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55638a526000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b896e7c 0x7f804b897aaf 0x7f804b939470 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x5562acce19ee 0x5562acc74bda 0x5562acce6d00 0x5562acce19ee 0x5562accea68c 0x7f7df5cae00d 0x7f7df5cb4dd7 0x7f7df5cb8ea8 0x5562acc73c52 0x5562acce6c25 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cc0dc3 0x5562acbb42eb 0x7f7df6085cbe 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce1ced 0x5562acc74bda\n",
            "[2022-01-05 16:54:24,301] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55638a526000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b892c7b 0x7f804b89335f 0x7f804b935103 0x5562acbb42eb 0x7f8008ffca1c 0x7f800904831c 0x7f800900016e 0x5562acc734b0 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc739bd 0x5562acde42f9 0x7f8009296502 0x7f800929980d 0x7f8009297275 0x7f8009297a1e 0x5562acc732ed 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4dd7 0x7f7df5cb8062 0x5562acc73c52 0x5562acce6c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x55645f998000 @  0x7f804dd432a4 0x7f800929b8bd 0x7f800929aa9d 0x7f8009297275 0x7f8009297a1e 0x5562acc732ed 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4dd7 0x7f7df5cb8062 0x5562acc73c52 0x5562acce6c25 0x5562acc74afa 0x5562acce2915 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4ab0 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55638a526000 @  0x7f804dd421e7 0x5562acca4f98 0x5562acc6fe27 0x7f8009297319 0x7f8009297a1e 0x5562acc732ed 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4dd7 0x7f7df5cb8062 0x5562acc73c52 0x5562acce6c25 0x5562acc74afa 0x5562acce2915 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4ab0 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x5565dd55e000 @  0x7f804dd432a4 0x7f8009298fd3 0x7f800929aa9d 0x7f8009297275 0x7f8009297a1e 0x5562acc732ed 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x5562acc74afa 0x5562acce6d00 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4ab0 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x55645f998000 @  0x7f804dd421e7 0x5562acca4f98 0x5562acc6fe27 0x7f8009297319 0x7f8009297a1e 0x5562acc732ed 0x5562acd64e1d 0x5562acce6e99 0x5562acce19ee 0x5562acc74bda 0x5562acce3737 0x5562acc74afa 0x5562acce6d00 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cb4ab0 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737 0x5562acce1ced 0x5562acc74bda 0x5562acce3737\n",
            "[2022-01-05 16:54:33,291] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 16:54:47,182] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55645f998000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b892c7b 0x7f804b89335f 0x7f804b935103 0x5562acbb42eb 0x7f8008ffca1c 0x7f800904831c 0x7f800900016e 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce19ee 0x5562acc74bda 0x5562acce6d00 0x5562acce19ee 0x5562accea68c 0x7f7df5cae00d 0x7f7df5cb2652 0x7f7df5cbe79b 0x7f7df5cbf4f6 0x5562acc73c52 0x5562acce6c25 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cc00b1 0x5562acbb42eb 0x7f7df607354b 0x5562acc73544 0x5562acc73240 0x5562acce7627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55645f998000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b892c7b 0x7f804b89335f 0x7f804b935103 0x5562acbb42eb 0x7f8008ffca1c 0x7f800904831c 0x7f800900016e 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce19ee 0x5562acc74bda 0x5562acce6d00 0x5562acce19ee 0x5562accea68c 0x7f7df5cae00d 0x7f7df5cb2652 0x7f7df5cbe79b 0x7f7df5cbf4f6 0x5562acc73c52 0x5562acce6c25 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cc00b1 0x5562acbb42eb 0x7f7df607354b 0x5562acc73544 0x5562acc73240 0x5562acce7627\n",
            "  0     200         15.84   1200.40   56.31   71.88   46.28    0.56\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55645f998000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b892c7b 0x7f804b89335f 0x7f804b935103 0x5562acbb42eb 0x7f8008ffca1c 0x7f800904831c 0x7f800900016e 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce19ee 0x5562acc74bda 0x5562acce6d00 0x5562acce19ee 0x5562accea68c 0x7f7df5cae00d 0x7f7df5cb2652 0x7f7df5cbe79b 0x7f7df5cbf4f6 0x5562acc73c52 0x5562acce6c25 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cc00b1 0x5562acbb42eb 0x7f7df607354b 0x5562acc73544 0x5562acc73240 0x5562acce7627\n",
            "  0     400         13.88    496.45   72.18   71.70   72.67    0.72\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55645f998000 @  0x7f804dd421e7 0x7f804b84246e 0x7f804b892c7b 0x7f804b89335f 0x7f804b935103 0x5562acbb42eb 0x7f8008ffca1c 0x7f800904831c 0x7f800900016e 0x5562acc73544 0x5562acc73240 0x5562acce7627 0x5562acce19ee 0x5562acc74bda 0x5562acce6d00 0x5562acce19ee 0x5562accea68c 0x7f7df5cae00d 0x7f7df5cb2652 0x7f7df5cbe79b 0x7f7df5cbf4f6 0x5562acc73c52 0x5562acce6c25 0x7f7df5cac148 0x7f7df5cae0ee 0x7f7df5cc00b1 0x5562acbb42eb 0x7f7df607354b 0x5562acc73544 0x5562acc73240 0x5562acce7627\n",
            "  0     600        621.92    705.36   74.55   79.53   70.16    0.75\n",
            "  0     800         47.28    763.15   75.46   75.00   75.93    0.75\n",
            "  0    1000         79.86    748.77   76.57   75.76   77.40    0.77\n",
            "  1    1200         43.11    702.07   77.38   78.69   76.12    0.77\n",
            "  1    1400         62.98    797.10   78.92   79.93   77.95    0.79\n",
            "  2    1600         58.00    601.16   80.05   79.01   81.12    0.80\n",
            "  3    1800         85.33    740.88   78.85   80.61   77.17    0.79\n",
            "  3    2000         77.12    530.20   79.91   81.33   78.53    0.80\n",
            "  4    2200         81.77    464.51   81.28   82.85   79.77    0.81\n",
            "  6    2400         88.84    351.96   80.76   82.16   79.42    0.81\n",
            "  7    2600        112.31    330.60   79.58   80.49   78.68    0.80\n",
            "  8    2800         68.18    173.37   81.15   83.33   79.07    0.81\n",
            "  9    3000        111.01    184.26   80.55   82.01   79.15    0.81\n",
            " 11    3200         69.48    110.57   79.05   80.89   77.29    0.79\n",
            " 12    3400         97.96    131.92   80.90   80.30   81.51    0.81\n",
            " 13    3600        126.38    125.78   79.93   81.93   78.02    0.80\n",
            " 14    3800         94.19    103.72   81.73   83.13   80.39    0.82\n",
            " 15    4000         70.13     83.42   80.11   81.01   79.22    0.80\n",
            " 17    4200        142.28    110.71   79.82   81.03   78.64    0.80\n",
            " 18    4400        169.48    128.41   80.79   82.25   79.38    0.81\n",
            " 19    4600        108.81     95.53   81.02   82.35   79.73    0.81\n",
            " 20    4800         91.53     84.15   81.26   82.90   79.69    0.81\n",
            " 22    5000         91.42     77.55   81.00   82.03   80.00    0.81\n",
            " 23    5200         80.54     60.12   80.90   81.11   80.70    0.81\n",
            " 24    5400        105.49     63.82   80.82   81.79   79.88    0.81\n",
            " 25    5600         57.06     40.81   80.90   83.12   78.80    0.81\n",
            " 27    5800         71.88     49.99   80.05   82.40   77.83    0.80\n",
            " 28    6000        120.19     87.65   79.96   82.43   77.64    0.80\n",
            " 29    6200         97.37     68.17   81.32   82.24   80.43    0.81\n",
            " 30    6400        106.71     68.48   80.95   81.28   80.62    0.81\n",
            " 32    6600        113.03     65.48   80.02   82.13   78.02    0.80\n",
            " 33    6800        155.33     90.84   78.97   79.27   78.68    0.79\n",
            " 34    7000         86.68     42.16   81.08   82.20   80.00    0.81\n",
            " 35    7200        108.25     77.94   78.24   80.27   76.32    0.78\n",
            " 37    7400        215.31    107.63   78.65   80.82   76.59    0.79\n",
            " 38    7600        364.83     73.11   81.17   82.96   79.46    0.81\n",
            " 39    7800        295.08    117.54   80.02   82.65   77.56    0.80\n",
            " 40    8000        126.51     75.00   78.92   81.44   76.55    0.79\n",
            " 42    8200        116.88     72.85   80.51   81.47   79.57    0.81\n",
            " 43    8400        108.74     62.81   80.39   82.13   78.72    0.80\n",
            " 44    8600        265.35    105.81   78.85   79.30   78.41    0.79\n",
            " 45    8800        160.36    106.36   80.42   82.19   78.72    0.80\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 18:04:04,227] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5594a17b2000 @  0x7fb412541001 0x7fb41003f54f 0x7fb41008fb58 0x7fb410093b17 0x7fb410132203 0x5594979f9544 0x5594979f9240 0x559497a6d627 0x559497a67ced 0x5594979fabda 0x559497a69737 0x559497a67ced 0x5594979fabda 0x559497a69737 0x559497a679ee 0x559497939e2b 0x559497a69fe4 0x559497a67ced 0x559497939e2b 0x559497a69fe4 0x559497a679ee 0x5594979fb48c 0x5594979fb698 0x559497a69fe4 0x5594979faafa 0x559497a68c0d 0x559497a67ced 0x5594979fabda 0x559497a68c0d 0x559497a67ced 0x5594979fabda\n",
            "2665029it [03:00, 14727.40it/s]\n",
            "[2022-01-05 18:07:06,729] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 18:08:48,263] [INFO] Set up nlp object from config\n",
            "[2022-01-05 18:08:48,272] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 18:08:48,276] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dcdbc7e000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11848e7c 0x7ffa11849aaf 0x7ffa118eb470 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd8d00 0x55dbeadd39ee 0x55dbeaddc68c 0x7ff7bbc6f00d 0x7ff7bbc75dd7 0x7ff7bbc79ea8 0x55dbead65c52 0x55dbeadd8c25 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc81dc3 0x55dbeaca62eb 0x7ff7bc046cbe 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd3ced 0x55dbead66bda\n",
            "[2022-01-05 18:09:02,261] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dcdbc7e000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11844c7b 0x7ffa1184535f 0x7ffa118e7103 0x55dbeaca62eb 0x7ff9cefaea1c 0x7ff9ceffa31c 0x7ff9cefb216e 0x55dbead654b0 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead659bd 0x55dbeaed62f9 0x7ff9cf248502 0x7ff9cf24b80d 0x7ff9cf249275 0x7ff9cf249a1e 0x55dbead652ed 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75dd7 0x7ff7bbc79062 0x55dbead65c52 0x55dbeadd8c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x55dd9afd0000 @  0x7ffa13cf52a4 0x7ff9cf24d8bd 0x7ff9cf24ca9d 0x7ff9cf249275 0x7ff9cf249a1e 0x55dbead652ed 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75dd7 0x7ff7bbc79062 0x55dbead65c52 0x55dbeadd8c25 0x55dbead66afa 0x55dbeadd4915 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75ab0 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dcdbc7e000 @  0x7ffa13cf41e7 0x55dbead96f98 0x55dbead61e27 0x7ff9cf249319 0x7ff9cf249a1e 0x55dbead652ed 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75dd7 0x7ff7bbc79062 0x55dbead65c52 0x55dbeadd8c25 0x55dbead66afa 0x55dbeadd4915 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75ab0 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x55df18b96000 @  0x7ffa13cf52a4 0x7ff9cf24afd3 0x7ff9cf24ca9d 0x7ff9cf249275 0x7ff9cf249a1e 0x55dbead652ed 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x55dbead66afa 0x55dbeadd8d00 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75ab0 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x55dd9afd0000 @  0x7ffa13cf41e7 0x55dbead96f98 0x55dbead61e27 0x7ff9cf249319 0x7ff9cf249a1e 0x55dbead652ed 0x55dbeae56e1d 0x55dbeadd8e99 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd5737 0x55dbead66afa 0x55dbeadd8d00 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc75ab0 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737 0x55dbeadd3ced 0x55dbead66bda 0x55dbeadd5737\n",
            "[2022-01-05 18:09:11,222] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 18:09:25,157] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dd9afd0000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11844c7b 0x7ffa1184535f 0x7ffa118e7103 0x55dbeaca62eb 0x7ff9cefaea1c 0x7ff9ceffa31c 0x7ff9cefb216e 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd8d00 0x55dbeadd39ee 0x55dbeaddc68c 0x7ff7bbc6f00d 0x7ff7bbc73652 0x7ff7bbc7f79b 0x7ff7bbc804f6 0x55dbead65c52 0x55dbeadd8c25 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc810b1 0x55dbeaca62eb 0x7ff7bc03454b 0x55dbead65544 0x55dbead65240 0x55dbeadd9627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dd9afd0000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11844c7b 0x7ffa1184535f 0x7ffa118e7103 0x55dbeaca62eb 0x7ff9cefaea1c 0x7ff9ceffa31c 0x7ff9cefb216e 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd8d00 0x55dbeadd39ee 0x55dbeaddc68c 0x7ff7bbc6f00d 0x7ff7bbc73652 0x7ff7bbc7f79b 0x7ff7bbc804f6 0x55dbead65c52 0x55dbeadd8c25 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc810b1 0x55dbeaca62eb 0x7ff7bc03454b 0x55dbead65544 0x55dbead65240 0x55dbeadd9627\n",
            "  0     200          5.69   1157.21   69.83   77.87   63.29    0.70\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dd9afd0000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11844c7b 0x7ffa1184535f 0x7ffa118e7103 0x55dbeaca62eb 0x7ff9cefaea1c 0x7ff9ceffa31c 0x7ff9cefb216e 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd8d00 0x55dbeadd39ee 0x55dbeaddc68c 0x7ff7bbc6f00d 0x7ff7bbc73652 0x7ff7bbc7f79b 0x7ff7bbc804f6 0x55dbead65c52 0x55dbeadd8c25 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc810b1 0x55dbeaca62eb 0x7ff7bc03454b 0x55dbead65544 0x55dbead65240 0x55dbeadd9627\n",
            "  0     400        109.89    605.46   68.02   71.19   65.12    0.68\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55dd9afd0000 @  0x7ffa13cf41e7 0x7ffa117f446e 0x7ffa11844c7b 0x7ffa1184535f 0x7ffa118e7103 0x55dbeaca62eb 0x7ff9cefaea1c 0x7ff9ceffa31c 0x7ff9cefb216e 0x55dbead65544 0x55dbead65240 0x55dbeadd9627 0x55dbeadd39ee 0x55dbead66bda 0x55dbeadd8d00 0x55dbeadd39ee 0x55dbeaddc68c 0x7ff7bbc6f00d 0x7ff7bbc73652 0x7ff7bbc7f79b 0x7ff7bbc804f6 0x55dbead65c52 0x55dbeadd8c25 0x7ff7bbc6d148 0x7ff7bbc6f0ee 0x7ff7bbc810b1 0x55dbeaca62eb 0x7ff7bc03454b 0x55dbead65544 0x55dbead65240 0x55dbeadd9627\n",
            "  0     600         47.23    664.21   74.51   77.19   72.02    0.75\n",
            "  0     800        182.00    849.26   76.96   78.09   75.85    0.77\n",
            "  0    1000         39.66    713.60   76.64   75.85   77.44    0.77\n",
            "  1    1200        425.61    770.80   78.34   78.59   78.10    0.78\n",
            "  1    1400         54.61    729.67   81.09   81.17   81.01    0.81\n",
            "  2    1600         60.98    640.98   78.35   77.87   78.84    0.78\n",
            "  3    1800         70.75    719.58   79.54   81.16   77.98    0.80\n",
            "  3    2000         85.27    617.86   82.19   83.86   80.58    0.82\n",
            "  4    2200        108.37    568.49   80.08   81.36   78.84    0.80\n",
            "  6    2400         84.38    440.34   81.58   82.94   80.27    0.82\n",
            "  7    2600        133.51    338.29   80.59   82.24   78.99    0.81\n",
            "  8    2800        110.71    248.71   80.38   81.46   79.34    0.80\n",
            "  9    3000        170.77    242.49   80.97   81.89   80.08    0.81\n",
            " 11    3200        150.90    211.29   80.40   81.65   79.19    0.80\n",
            " 12    3400        108.07    142.28   80.78   81.85   79.73    0.81\n",
            " 13    3600         71.50     85.01   80.00   81.53   78.53    0.80\n",
            " 14    3800         78.17     95.96   80.26   82.46   78.18    0.80\n",
            " 15    4000         49.81     58.15   79.36   82.42   76.51    0.79\n",
            " 17    4200         99.35     78.39   79.83   81.93   77.83    0.80\n",
            " 18    4400        105.39     89.62   80.14   81.32   78.99    0.80\n",
            " 19    4600         89.61     79.13   80.09   81.50   78.72    0.80\n",
            " 20    4800        149.02     89.26   80.17   80.82   79.53    0.80\n",
            " 22    5000         75.57     69.35   78.41   79.89   76.98    0.78\n",
            " 23    5200         68.95     55.73   79.08   81.69   76.63    0.79\n",
            " 24    5400        103.96     84.78   81.71   83.41   80.08    0.82\n",
            " 25    5600         91.54     64.30   80.69   83.05   78.45    0.81\n",
            " 27    5800        108.44     91.44   80.42   81.60   79.26    0.80\n",
            " 28    6000        373.96    109.50   77.79   80.40   75.35    0.78\n",
            " 29    6200        130.53    104.37   80.55   82.46   78.72    0.81\n",
            " 30    6400        239.83    147.02   80.79   81.91   79.69    0.81\n",
            " 32    6600        206.00    117.45   78.80   80.76   76.94    0.79\n",
            " 33    6800        103.18     81.47   80.53   82.34   78.80    0.81\n",
            " 34    7000        112.28     56.69   79.23   79.43   79.03    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..5.wordngram-5.neg_sampling-15/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 19:04:46,021] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5636af738000 @  0x7f7eb621a001 0x7f7eb3d1854f 0x7f7eb3d68b58 0x7f7eb3d6cb17 0x7f7eb3e0b203 0x5636a5db3544 0x5636a5db3240 0x5636a5e27627 0x5636a5e21ced 0x5636a5db4bda 0x5636a5e23737 0x5636a5e21ced 0x5636a5db4bda 0x5636a5e23737 0x5636a5e219ee 0x5636a5cf3e2b 0x5636a5e23fe4 0x5636a5e21ced 0x5636a5cf3e2b 0x5636a5e23fe4 0x5636a5e219ee 0x5636a5db548c 0x5636a5db5698 0x5636a5e23fe4 0x5636a5db4afa 0x5636a5e22c0d 0x5636a5e21ced 0x5636a5db4bda 0x5636a5e22c0d 0x5636a5e21ced 0x5636a5db4bda\n",
            "2665029it [03:02, 14587.92it/s]\n",
            "[2022-01-05 19:07:50,288] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 19:09:29,599] [INFO] Set up nlp object from config\n",
            "[2022-01-05 19:09:29,609] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 19:09:29,613] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564be3c34000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49d3e7c 0x7fa4e49d4aaf 0x7fa4e4a76470 0x564af5497544 0x564af5497240 0x564af550b627 0x564af55059ee 0x564af5498bda 0x564af5507737 0x564af55059ee 0x564af5498bda 0x564af550ad00 0x564af55059ee 0x564af550e68c 0x7fa28edee00d 0x7fa28edf4dd7 0x7fa28edf8ea8 0x564af5497c52 0x564af550ac25 0x7fa28edec148 0x7fa28edee0ee 0x7fa28ee00dc3 0x564af53d82eb 0x7fa28f1c5cbe 0x564af5497544 0x564af5497240 0x564af550b627 0x564af5505ced 0x564af5498bda\n",
            "[2022-01-05 19:09:43,522] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564be3c34000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49cfc7b 0x7fa4e49d035f 0x7fa4e4a72103 0x564af53d82eb 0x7fa4a2139a1c 0x7fa4a218531c 0x7fa4a213d16e 0x564af54974b0 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af54979bd 0x564af56082f9 0x7fa4a23d3502 0x7fa4a23d680d 0x7fa4a23d4275 0x7fa4a23d4a1e 0x564af54972ed 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af5498bda 0x564af5507737 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4dd7 0x7fa28edf8062 0x564af5497c52 0x564af550ac25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x564ca5f28000 @  0x7fa4e6e802a4 0x7fa4a23d88bd 0x7fa4a23d7a9d 0x7fa4a23d4275 0x7fa4a23d4a1e 0x564af54972ed 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af5498bda 0x564af5507737 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4dd7 0x7fa28edf8062 0x564af5497c52 0x564af550ac25 0x564af5498afa 0x564af5506915 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4ab0 0x564af5497544 0x564af5497240 0x564af550b627 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564be3c34000 @  0x7fa4e6e7f1e7 0x564af54c8f98 0x564af5493e27 0x7fa4a23d4319 0x7fa4a23d4a1e 0x564af54972ed 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af5498bda 0x564af5507737 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4dd7 0x7fa28edf8062 0x564af5497c52 0x564af550ac25 0x564af5498afa 0x564af5506915 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4ab0 0x564af5497544 0x564af5497240 0x564af550b627 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x564e23aee000 @  0x7fa4e6e802a4 0x7fa4a23d5fd3 0x7fa4a23d7a9d 0x7fa4a23d4275 0x7fa4a23d4a1e 0x564af54972ed 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af5498bda 0x564af5507737 0x564af5498afa 0x564af550ad00 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4ab0 0x564af5497544 0x564af5497240 0x564af550b627 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x564ca5f28000 @  0x7fa4e6e7f1e7 0x564af54c8f98 0x564af5493e27 0x7fa4a23d4319 0x7fa4a23d4a1e 0x564af54972ed 0x564af5588e1d 0x564af550ae99 0x564af55059ee 0x564af5498bda 0x564af5507737 0x564af5498afa 0x564af550ad00 0x7fa28edec148 0x7fa28edee0ee 0x7fa28edf4ab0 0x564af5497544 0x564af5497240 0x564af550b627 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737 0x564af5505ced 0x564af5498bda 0x564af5507737\n",
            "[2022-01-05 19:09:52,704] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 19:10:05,612] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564ca5f28000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49cfc7b 0x7fa4e49d035f 0x7fa4e4a72103 0x564af53d82eb 0x7fa4a2139a1c 0x7fa4a218531c 0x7fa4a213d16e 0x564af5497544 0x564af5497240 0x564af550b627 0x564af55059ee 0x564af5498bda 0x564af550ad00 0x564af55059ee 0x564af550e68c 0x7fa28edee00d 0x7fa28edf2652 0x7fa28edfe79b 0x7fa28edff4f6 0x564af5497c52 0x564af550ac25 0x7fa28edec148 0x7fa28edee0ee 0x7fa28ee000b1 0x564af53d82eb 0x7fa28f1b354b 0x564af5497544 0x564af5497240 0x564af550b627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564ca5f28000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49cfc7b 0x7fa4e49d035f 0x7fa4e4a72103 0x564af53d82eb 0x7fa4a2139a1c 0x7fa4a218531c 0x7fa4a213d16e 0x564af5497544 0x564af5497240 0x564af550b627 0x564af55059ee 0x564af5498bda 0x564af550ad00 0x564af55059ee 0x564af550e68c 0x7fa28edee00d 0x7fa28edf2652 0x7fa28edfe79b 0x7fa28edff4f6 0x564af5497c52 0x564af550ac25 0x7fa28edec148 0x7fa28edee0ee 0x7fa28ee000b1 0x564af53d82eb 0x7fa28f1b354b 0x564af5497544 0x564af5497240 0x564af550b627\n",
            "  0     200          5.33   1221.55   62.04   79.82   50.74    0.62\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564ca5f28000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49cfc7b 0x7fa4e49d035f 0x7fa4e4a72103 0x564af53d82eb 0x7fa4a2139a1c 0x7fa4a218531c 0x7fa4a213d16e 0x564af5497544 0x564af5497240 0x564af550b627 0x564af55059ee 0x564af5498bda 0x564af550ad00 0x564af55059ee 0x564af550e68c 0x7fa28edee00d 0x7fa28edf2652 0x7fa28edfe79b 0x7fa28edff4f6 0x564af5497c52 0x564af550ac25 0x7fa28edec148 0x7fa28edee0ee 0x7fa28ee000b1 0x564af53d82eb 0x7fa28f1b354b 0x564af5497544 0x564af5497240 0x564af550b627\n",
            "  0     400         35.55    552.97   69.54   69.79   69.30    0.70\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564ca5f28000 @  0x7fa4e6e7f1e7 0x7fa4e497f46e 0x7fa4e49cfc7b 0x7fa4e49d035f 0x7fa4e4a72103 0x564af53d82eb 0x7fa4a2139a1c 0x7fa4a218531c 0x7fa4a213d16e 0x564af5497544 0x564af5497240 0x564af550b627 0x564af55059ee 0x564af5498bda 0x564af550ad00 0x564af55059ee 0x564af550e68c 0x7fa28edee00d 0x7fa28edf2652 0x7fa28edfe79b 0x7fa28edff4f6 0x564af5497c52 0x564af550ac25 0x7fa28edec148 0x7fa28edee0ee 0x7fa28ee000b1 0x564af53d82eb 0x7fa28f1b354b 0x564af5497544 0x564af5497240 0x564af550b627\n",
            "  0     600         26.29    558.18   72.64   80.52   66.16    0.73\n",
            "  0     800         53.81    790.45   74.41   73.67   75.16    0.74\n",
            "  0    1000         41.88    698.75   75.12   74.77   75.47    0.75\n",
            "  1    1200         90.94    674.24   77.99   80.21   75.89    0.78\n",
            "  1    1400         65.48    753.35   79.11   80.31   77.95    0.79\n",
            "  2    1600         85.89    661.66   79.36   79.89   78.84    0.79\n",
            "  3    1800         83.58    779.24   79.05   81.03   77.17    0.79\n",
            "  3    2000        124.22    646.94   81.61   85.45   78.10    0.82\n",
            "  4    2200         74.13    506.67   81.51   83.50   79.61    0.82\n",
            "  6    2400         79.09    351.94   80.06   82.96   77.36    0.80\n",
            "  7    2600         71.53    237.34   80.21   81.51   78.95    0.80\n",
            "  8    2800         68.19    178.42   81.39   82.37   80.43    0.81\n",
            "  9    3000        101.90    203.55   79.42   79.70   79.15    0.79\n",
            " 11    3200         62.24    123.17   81.60   82.89   80.35    0.82\n",
            " 12    3400        182.40    197.19   80.09   80.37   79.81    0.80\n",
            " 13    3600         86.87     78.48   81.15   81.97   80.35    0.81\n",
            " 14    3800         58.59     79.88   78.77   79.61   77.95    0.79\n",
            " 15    4000         30.19     36.37   80.62   81.28   79.96    0.81\n",
            " 17    4200         73.84     70.05   78.45   79.85   77.09    0.78\n",
            " 18    4400        147.77    126.28   80.61   82.94   78.41    0.81\n",
            " 19    4600        130.81    109.17   79.24   82.27   76.43    0.79\n",
            " 20    4800         87.74     82.71   79.83   81.59   78.14    0.80\n",
            " 22    5000         69.04     66.17   80.73   82.12   79.38    0.81\n",
            " 23    5200        112.45     86.53   78.69   80.39   77.05    0.79\n",
            " 24    5400        138.71     90.71   80.55   81.34   79.77    0.81\n",
            " 25    5600        108.62     93.30   79.99   81.06   78.95    0.80\n",
            " 27    5800         83.44     77.66   78.21   81.57   75.12    0.78\n",
            " 28    6000        118.94    100.23   78.32   81.98   74.96    0.78\n",
            " 29    6200        149.85    101.75   79.77   82.11   77.56    0.80\n",
            " 30    6400        183.01     73.70   80.00   83.00   77.21    0.80\n",
            " 32    6600         45.56     37.15   80.73   82.54   78.99    0.81\n",
            " 33    6800         80.49     54.78   81.41   82.38   80.47    0.81\n",
            " 34    7000        124.76     56.43   80.30   82.85   77.91    0.80\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-15/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 20:08:19,666] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55ae66684000 @  0x7f4a52cc0001 0x7f4a507be54f 0x7f4a5080eb58 0x7f4a50812b17 0x7f4a508b1203 0x55ae5d3f2544 0x55ae5d3f2240 0x55ae5d466627 0x55ae5d460ced 0x55ae5d3f3bda 0x55ae5d462737 0x55ae5d460ced 0x55ae5d3f3bda 0x55ae5d462737 0x55ae5d4609ee 0x55ae5d332e2b 0x55ae5d462fe4 0x55ae5d460ced 0x55ae5d332e2b 0x55ae5d462fe4 0x55ae5d4609ee 0x55ae5d3f448c 0x55ae5d3f4698 0x55ae5d462fe4 0x55ae5d3f3afa 0x55ae5d461c0d 0x55ae5d460ced 0x55ae5d3f3bda 0x55ae5d461c0d 0x55ae5d460ced 0x55ae5d3f3bda\n",
            "2665029it [03:02, 14627.53it/s]\n",
            "[2022-01-05 20:11:23,494] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 20:13:05,071] [INFO] Set up nlp object from config\n",
            "[2022-01-05 20:13:05,080] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 20:13:05,084] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563a24af0000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796fce7c 0x7f56796fdaaf 0x7f567979f470 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x563935fba9ee 0x563935f4dbda 0x563935fbfd00 0x563935fba9ee 0x563935fc368c 0x7f5423b1400d 0x7f5423b1add7 0x7f5423b1eea8 0x563935f4cc52 0x563935fbfc25 0x7f5423b12148 0x7f5423b140ee 0x7f5423b26dc3 0x563935e8d2eb 0x7f5423eebcbe 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fbaced 0x563935f4dbda\n",
            "[2022-01-05 20:13:19,141] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563a24af0000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796f8c7b 0x7f56796f935f 0x7f567979b103 0x563935e8d2eb 0x7f5636e62a1c 0x7f5636eae31c 0x7f5636e6616e 0x563935f4c4b0 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4c9bd 0x5639360bd2f9 0x7f56370fc502 0x7f56370ff80d 0x7f56370fd275 0x7f56370fda1e 0x563935f4c2ed 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1add7 0x7f5423b1e062 0x563935f4cc52 0x563935fbfc25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x563ae6df6000 @  0x7f567bba92a4 0x7f56371018bd 0x7f5637100a9d 0x7f56370fd275 0x7f56370fda1e 0x563935f4c2ed 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1add7 0x7f5423b1e062 0x563935f4cc52 0x563935fbfc25 0x563935f4dafa 0x563935fbb915 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1aab0 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563a24af0000 @  0x7f567bba81e7 0x563935f7df98 0x563935f48e27 0x7f56370fd319 0x7f56370fda1e 0x563935f4c2ed 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1add7 0x7f5423b1e062 0x563935f4cc52 0x563935fbfc25 0x563935f4dafa 0x563935fbb915 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1aab0 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x563c649bc000 @  0x7f567bba92a4 0x7f56370fefd3 0x7f5637100a9d 0x7f56370fd275 0x7f56370fda1e 0x563935f4c2ed 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x563935f4dafa 0x563935fbfd00 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1aab0 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x563ae6df6000 @  0x7f567bba81e7 0x563935f7df98 0x563935f48e27 0x7f56370fd319 0x7f56370fda1e 0x563935f4c2ed 0x56393603de1d 0x563935fbfe99 0x563935fba9ee 0x563935f4dbda 0x563935fbc737 0x563935f4dafa 0x563935fbfd00 0x7f5423b12148 0x7f5423b140ee 0x7f5423b1aab0 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737 0x563935fbaced 0x563935f4dbda 0x563935fbc737\n",
            "[2022-01-05 20:13:28,430] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 20:13:41,581] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563ae6df6000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796f8c7b 0x7f56796f935f 0x7f567979b103 0x563935e8d2eb 0x7f5636e62a1c 0x7f5636eae31c 0x7f5636e6616e 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fba9ee 0x563935f4dbda 0x563935fbfd00 0x563935fba9ee 0x563935fc368c 0x7f5423b1400d 0x7f5423b18652 0x7f5423b2479b 0x7f5423b254f6 0x563935f4cc52 0x563935fbfc25 0x7f5423b12148 0x7f5423b140ee 0x7f5423b260b1 0x563935e8d2eb 0x7f5423ed954b 0x563935f4c544 0x563935f4c240 0x563935fc0627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563ae6df6000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796f8c7b 0x7f56796f935f 0x7f567979b103 0x563935e8d2eb 0x7f5636e62a1c 0x7f5636eae31c 0x7f5636e6616e 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fba9ee 0x563935f4dbda 0x563935fbfd00 0x563935fba9ee 0x563935fc368c 0x7f5423b1400d 0x7f5423b18652 0x7f5423b2479b 0x7f5423b254f6 0x563935f4cc52 0x563935fbfc25 0x7f5423b12148 0x7f5423b140ee 0x7f5423b260b1 0x563935e8d2eb 0x7f5423ed954b 0x563935f4c544 0x563935f4c240 0x563935fc0627\n",
            "  0     200          6.02   1172.25   67.67   73.81   62.48    0.68\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563ae6df6000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796f8c7b 0x7f56796f935f 0x7f567979b103 0x563935e8d2eb 0x7f5636e62a1c 0x7f5636eae31c 0x7f5636e6616e 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fba9ee 0x563935f4dbda 0x563935fbfd00 0x563935fba9ee 0x563935fc368c 0x7f5423b1400d 0x7f5423b18652 0x7f5423b2479b 0x7f5423b254f6 0x563935f4cc52 0x563935fbfc25 0x7f5423b12148 0x7f5423b140ee 0x7f5423b260b1 0x563935e8d2eb 0x7f5423ed954b 0x563935f4c544 0x563935f4c240 0x563935fc0627\n",
            "  0     400        187.09    640.31   71.26   72.04   70.50    0.71\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563ae6df6000 @  0x7f567bba81e7 0x7f56796a846e 0x7f56796f8c7b 0x7f56796f935f 0x7f567979b103 0x563935e8d2eb 0x7f5636e62a1c 0x7f5636eae31c 0x7f5636e6616e 0x563935f4c544 0x563935f4c240 0x563935fc0627 0x563935fba9ee 0x563935f4dbda 0x563935fbfd00 0x563935fba9ee 0x563935fc368c 0x7f5423b1400d 0x7f5423b18652 0x7f5423b2479b 0x7f5423b254f6 0x563935f4cc52 0x563935fbfc25 0x7f5423b12148 0x7f5423b140ee 0x7f5423b260b1 0x563935e8d2eb 0x7f5423ed954b 0x563935f4c544 0x563935f4c240 0x563935fc0627\n",
            "  0     600        114.22    629.76   73.06   77.35   69.22    0.73\n",
            "  0     800         40.67    723.22   75.35   75.23   75.47    0.75\n",
            "  0    1000         42.29    722.47   75.31   74.28   76.36    0.75\n",
            "  1    1200         62.44    770.54   75.59   77.46   73.80    0.76\n",
            "  1    1400         73.56    777.37   78.77   79.38   78.18    0.79\n",
            "  2    1600         67.12    663.71   79.69   80.55   78.84    0.80\n",
            "  3    1800         92.79    739.30   79.60   80.82   78.41    0.80\n",
            "  3    2000         77.13    544.31   80.50   83.35   77.83    0.80\n",
            "  4    2200         97.59    618.02   81.79   83.29   80.35    0.82\n",
            "  6    2400         95.18    461.17   80.58   82.10   79.11    0.81\n",
            "  7    2600         73.29    258.66   81.27   81.90   80.66    0.81\n",
            "  8    2800        123.97    250.26   81.42   82.27   80.58    0.81\n",
            "  9    3000        103.41    153.18   81.25   81.97   80.54    0.81\n",
            " 11    3200        120.61    128.82   81.54   82.93   80.19    0.82\n",
            " 12    3400         88.26    109.79   80.94   83.24   78.76    0.81\n",
            " 13    3600         86.35    104.64   80.95   83.10   78.91    0.81\n",
            " 14    3800        102.11    102.13   80.52   81.17   79.88    0.81\n",
            " 15    4000         69.53     80.02   81.58   82.68   80.50    0.82\n",
            " 17    4200         75.72     67.27   80.17   82.35   78.10    0.80\n",
            " 18    4400         85.99     64.54   80.33   83.04   77.79    0.80\n",
            " 19    4600         87.02     76.26   81.09   81.68   80.50    0.81\n",
            " 20    4800         82.47     75.95   80.86   82.02   79.73    0.81\n",
            " 22    5000         63.06     51.21   80.68   81.85   79.53    0.81\n",
            " 23    5200         92.31     66.52   80.74   83.42   78.22    0.81\n",
            " 24    5400        112.34     76.88   78.93   79.25   78.60    0.79\n",
            " 25    5600        210.81    124.46   79.15   80.82   77.56    0.79\n",
            " 27    5800        182.73    129.71   81.12   83.33   79.03    0.81\n",
            " 28    6000         95.53     65.86   78.77   81.25   76.43    0.79\n",
            " 29    6200         95.84     65.44   80.36   82.23   78.57    0.80\n",
            " 30    6400        192.60     90.73   79.49   81.82   77.29    0.79\n",
            " 32    6600        126.46     54.16   79.69   80.80   78.60    0.80\n",
            " 33    6800        126.90     70.03   80.67   82.30   79.11    0.81\n",
            " 34    7000         73.71     39.57   78.96   80.61   77.36    0.79\n",
            " 35    7200        115.70     61.69   78.91   81.41   76.55    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 21:12:34,090] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55d8f2c94000 @  0x7f647fb99001 0x7f647d69754f 0x7f647d6e7b58 0x7f647d6ebb17 0x7f647d78a203 0x55d8e9569544 0x55d8e9569240 0x55d8e95dd627 0x55d8e95d7ced 0x55d8e956abda 0x55d8e95d9737 0x55d8e95d7ced 0x55d8e956abda 0x55d8e95d9737 0x55d8e95d79ee 0x55d8e94a9e2b 0x55d8e95d9fe4 0x55d8e95d7ced 0x55d8e94a9e2b 0x55d8e95d9fe4 0x55d8e95d79ee 0x55d8e956b48c 0x55d8e956b698 0x55d8e95d9fe4 0x55d8e956aafa 0x55d8e95d8c0d 0x55d8e95d7ced 0x55d8e956abda 0x55d8e95d8c0d 0x55d8e95d7ced 0x55d8e956abda\n",
            "2665029it [02:58, 14930.49it/s]\n",
            "[2022-01-05 21:15:34,165] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 21:17:18,526] [INFO] Set up nlp object from config\n",
            "[2022-01-05 21:17:18,536] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 21:17:18,540] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5607c6db8000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c9e7c 0x7fde031caaaf 0x7fde0326c470 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x5606d554e9ee 0x5606d54e1bda 0x5606d5553d00 0x5606d554e9ee 0x5606d555768c 0x7fdbad5e600d 0x7fdbad5ecdd7 0x7fdbad5f0ea8 0x5606d54e0c52 0x5606d5553c25 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5f8dc3 0x5606d54212eb 0x7fdbad9bdcbe 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554eced 0x5606d54e1bda\n",
            "[2022-01-05 21:17:32,516] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5607c6db8000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c5c7b 0x7fde031c635f 0x7fde03268103 0x5606d54212eb 0x7fddc092fa1c 0x7fddc097b31c 0x7fddc093316e 0x5606d54e04b0 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e09bd 0x5606d56512f9 0x7fddc0bc9502 0x7fddc0bcc80d 0x7fddc0bca275 0x7fddc0bcaa1e 0x5606d54e02ed 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecdd7 0x7fdbad5f0062 0x5606d54e0c52 0x5606d5553c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x560885ff6000 @  0x7fde056762a4 0x7fddc0bce8bd 0x7fddc0bcda9d 0x7fddc0bca275 0x7fddc0bcaa1e 0x5606d54e02ed 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecdd7 0x7fdbad5f0062 0x5606d54e0c52 0x5606d5553c25 0x5606d54e1afa 0x5606d554f915 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecab0 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5607c6db8000 @  0x7fde056751e7 0x5606d5511f98 0x5606d54dce27 0x7fddc0bca319 0x7fddc0bcaa1e 0x5606d54e02ed 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecdd7 0x7fdbad5f0062 0x5606d54e0c52 0x5606d5553c25 0x5606d54e1afa 0x5606d554f915 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecab0 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x560a03bbc000 @  0x7fde056762a4 0x7fddc0bcbfd3 0x7fddc0bcda9d 0x7fddc0bca275 0x7fddc0bcaa1e 0x5606d54e02ed 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x5606d54e1afa 0x5606d5553d00 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecab0 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x560885ff6000 @  0x7fde056751e7 0x5606d5511f98 0x5606d54dce27 0x7fddc0bca319 0x7fddc0bcaa1e 0x5606d54e02ed 0x5606d55d1e1d 0x5606d5553e99 0x5606d554e9ee 0x5606d54e1bda 0x5606d5550737 0x5606d54e1afa 0x5606d5553d00 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5ecab0 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737 0x5606d554eced 0x5606d54e1bda 0x5606d5550737\n",
            "[2022-01-05 21:17:41,728] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 21:17:54,369] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x560885ff6000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c5c7b 0x7fde031c635f 0x7fde03268103 0x5606d54212eb 0x7fddc092fa1c 0x7fddc097b31c 0x7fddc093316e 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554e9ee 0x5606d54e1bda 0x5606d5553d00 0x5606d554e9ee 0x5606d555768c 0x7fdbad5e600d 0x7fdbad5ea652 0x7fdbad5f679b 0x7fdbad5f74f6 0x5606d54e0c52 0x5606d5553c25 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5f80b1 0x5606d54212eb 0x7fdbad9ab54b 0x5606d54e0544 0x5606d54e0240 0x5606d5554627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x560885ff6000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c5c7b 0x7fde031c635f 0x7fde03268103 0x5606d54212eb 0x7fddc092fa1c 0x7fddc097b31c 0x7fddc093316e 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554e9ee 0x5606d54e1bda 0x5606d5553d00 0x5606d554e9ee 0x5606d555768c 0x7fdbad5e600d 0x7fdbad5ea652 0x7fdbad5f679b 0x7fdbad5f74f6 0x5606d54e0c52 0x5606d5553c25 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5f80b1 0x5606d54212eb 0x7fdbad9ab54b 0x5606d54e0544 0x5606d54e0240 0x5606d5554627\n",
            "  0     200          5.97   1147.51   67.65   73.80   62.44    0.68\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x560885ff6000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c5c7b 0x7fde031c635f 0x7fde03268103 0x5606d54212eb 0x7fddc092fa1c 0x7fddc097b31c 0x7fddc093316e 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554e9ee 0x5606d54e1bda 0x5606d5553d00 0x5606d554e9ee 0x5606d555768c 0x7fdbad5e600d 0x7fdbad5ea652 0x7fdbad5f679b 0x7fdbad5f74f6 0x5606d54e0c52 0x5606d5553c25 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5f80b1 0x5606d54212eb 0x7fdbad9ab54b 0x5606d54e0544 0x5606d54e0240 0x5606d5554627\n",
            "  0     400         78.43    578.76   70.60   71.32   69.88    0.71\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x560885ff6000 @  0x7fde056751e7 0x7fde0317546e 0x7fde031c5c7b 0x7fde031c635f 0x7fde03268103 0x5606d54212eb 0x7fddc092fa1c 0x7fddc097b31c 0x7fddc093316e 0x5606d54e0544 0x5606d54e0240 0x5606d5554627 0x5606d554e9ee 0x5606d54e1bda 0x5606d5553d00 0x5606d554e9ee 0x5606d555768c 0x7fdbad5e600d 0x7fdbad5ea652 0x7fdbad5f679b 0x7fdbad5f74f6 0x5606d54e0c52 0x5606d5553c25 0x7fdbad5e4148 0x7fdbad5e60ee 0x7fdbad5f80b1 0x5606d54212eb 0x7fdbad9ab54b 0x5606d54e0544 0x5606d54e0240 0x5606d5554627\n",
            "  0     600         28.57    597.05   74.25   82.24   67.67    0.74\n",
            "  0     800         46.36    717.90   76.75   78.20   75.35    0.77\n",
            "  0    1000         38.96    726.57   76.87   78.32   75.47    0.77\n",
            "  1    1200         86.75    864.71   77.29   79.08   75.58    0.77\n",
            "  1    1400         57.37    786.16   80.14   82.00   78.37    0.80\n",
            "  2    1600         57.90    630.39   79.39   79.13   79.65    0.79\n",
            "  3    1800         80.27    738.56   81.32   83.27   79.46    0.81\n",
            "  3    2000         82.33    555.52   79.24   81.20   77.36    0.79\n",
            "  4    2200         85.21    538.31   80.95   82.21   79.73    0.81\n",
            "  6    2400         74.05    399.30   81.53   84.68   78.60    0.82\n",
            "  7    2600        211.32    357.14   80.32   81.68   78.99    0.80\n",
            "  8    2800         69.84    178.72   81.78   83.23   80.39    0.82\n",
            "  9    3000         97.14    151.03   79.89   81.05   78.76    0.80\n",
            " 11    3200         94.43    138.88   81.38   82.57   80.23    0.81\n",
            " 12    3400         81.09    103.63   81.23   82.95   79.57    0.81\n",
            " 13    3600        100.25    120.33   81.04   80.50   81.59    0.81\n",
            " 14    3800         98.26     90.73   80.81   81.32   80.31    0.81\n",
            " 15    4000        163.53    132.15   79.62   80.75   78.53    0.80\n",
            " 17    4200        147.55    128.82   81.70   82.36   81.05    0.82\n",
            " 18    4400       1116.16     95.73   80.89   82.08   79.73    0.81\n",
            " 19    4600        121.36    100.42   80.25   81.39   79.15    0.80\n",
            " 20    4800         75.63     73.09   80.73   82.18   79.34    0.81\n",
            " 22    5000        110.67     91.48   79.29   80.97   77.67    0.79\n",
            " 23    5200        123.92     74.70   80.59   82.13   79.11    0.81\n",
            " 24    5400        111.34     83.54   79.60   80.58   78.64    0.80\n",
            " 25    5600         96.73     82.94   80.65   81.95   79.38    0.81\n",
            " 27    5800         91.45     81.32   79.45   80.40   78.53    0.79\n",
            " 28    6000        110.52     80.97   79.62   81.58   77.75    0.80\n",
            " 29    6200        156.73     95.00   79.88   80.27   79.50    0.80\n",
            " 30    6400        157.45    100.12   79.61   80.77   78.49    0.80\n",
            " 32    6600        100.68     93.38   79.39   81.39   77.48    0.79\n",
            " 33    6800        117.25     75.06   80.43   81.39   79.50    0.80\n",
            " 34    7000        406.84     83.40   80.61   81.39   79.84    0.81\n",
            " 35    7200        150.19     83.75   79.91   82.24   77.71    0.80\n",
            " 37    7400         61.53     44.43   81.10   82.06   80.16    0.81\n",
            " 38    7600        142.85    102.70   80.02   81.37   78.72    0.80\n",
            " 39    7800         98.39     42.99   79.06   80.70   77.48    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-15/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 22:21:32,842] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55eb306e4000 @  0x7f706cd07001 0x7f706a80554f 0x7f706a855b58 0x7f706a859b17 0x7f706a8f8203 0x55eb25b32544 0x55eb25b32240 0x55eb25ba6627 0x55eb25ba0ced 0x55eb25b33bda 0x55eb25ba2737 0x55eb25ba0ced 0x55eb25b33bda 0x55eb25ba2737 0x55eb25ba09ee 0x55eb25a72e2b 0x55eb25ba2fe4 0x55eb25ba0ced 0x55eb25a72e2b 0x55eb25ba2fe4 0x55eb25ba09ee 0x55eb25b3448c 0x55eb25b34698 0x55eb25ba2fe4 0x55eb25b33afa 0x55eb25ba1c0d 0x55eb25ba0ced 0x55eb25b33bda 0x55eb25ba1c0d 0x55eb25ba0ced 0x55eb25b33bda\n",
            "2665029it [03:01, 14708.44it/s]\n",
            "[2022-01-05 22:24:35,613] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 22:26:11,283] [INFO] Set up nlp object from config\n",
            "[2022-01-05 22:26:11,292] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 22:26:11,296] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56043809a000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f473e7c 0x7f688f474aaf 0x7f688f516470 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x56035cb279ee 0x56035cababda 0x56035cb2cd00 0x56035cb279ee 0x56035cb3068c 0x7f663989600d 0x7f663989cdd7 0x7f66398a0ea8 0x56035cab9c52 0x56035cb2cc25 0x7f6639894148 0x7f66398960ee 0x7f66398a8dc3 0x56035c9fa2eb 0x7f6639c6dcbe 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb27ced 0x56035cababda\n",
            "[2022-01-05 22:26:25,066] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56043809a000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f46fc7b 0x7f688f47035f 0x7f688f512103 0x56035c9fa2eb 0x7f684cbd9a1c 0x7f684cc2531c 0x7f684cbdd16e 0x56035cab94b0 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cab99bd 0x56035cc2a2f9 0x7f684ce73502 0x7f684ce7680d 0x7f684ce74275 0x7f684ce74a1e 0x56035cab92ed 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x7f6639894148 0x7f66398960ee 0x7f663989cdd7 0x7f66398a0062 0x56035cab9c52 0x56035cb2cc25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x56050e8ec000 @  0x7f68919202a4 0x7f684ce788bd 0x7f684ce77a9d 0x7f684ce74275 0x7f684ce74a1e 0x56035cab92ed 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x7f6639894148 0x7f66398960ee 0x7f663989cdd7 0x7f66398a0062 0x56035cab9c52 0x56035cb2cc25 0x56035cabaafa 0x56035cb28915 0x7f6639894148 0x7f66398960ee 0x7f663989cab0 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56043809a000 @  0x7f689191f1e7 0x56035caeaf98 0x56035cab5e27 0x7f684ce74319 0x7f684ce74a1e 0x56035cab92ed 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x7f6639894148 0x7f66398960ee 0x7f663989cdd7 0x7f66398a0062 0x56035cab9c52 0x56035cb2cc25 0x56035cabaafa 0x56035cb28915 0x7f6639894148 0x7f66398960ee 0x7f663989cab0 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x56068c4b2000 @  0x7f68919202a4 0x7f684ce75fd3 0x7f684ce77a9d 0x7f684ce74275 0x7f684ce74a1e 0x56035cab92ed 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x56035cabaafa 0x56035cb2cd00 0x7f6639894148 0x7f66398960ee 0x7f663989cab0 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x56050e8ec000 @  0x7f689191f1e7 0x56035caeaf98 0x56035cab5e27 0x7f684ce74319 0x7f684ce74a1e 0x56035cab92ed 0x56035cbaae1d 0x56035cb2ce99 0x56035cb279ee 0x56035cababda 0x56035cb29737 0x56035cabaafa 0x56035cb2cd00 0x7f6639894148 0x7f66398960ee 0x7f663989cab0 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737 0x56035cb27ced 0x56035cababda 0x56035cb29737\n",
            "[2022-01-05 22:26:34,227] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 22:26:47,026] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56050e8ec000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f46fc7b 0x7f688f47035f 0x7f688f512103 0x56035c9fa2eb 0x7f684cbd9a1c 0x7f684cc2531c 0x7f684cbdd16e 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb279ee 0x56035cababda 0x56035cb2cd00 0x56035cb279ee 0x56035cb3068c 0x7f663989600d 0x7f663989a652 0x7f66398a679b 0x7f66398a74f6 0x56035cab9c52 0x56035cb2cc25 0x7f6639894148 0x7f66398960ee 0x7f66398a80b1 0x56035c9fa2eb 0x7f6639c5b54b 0x56035cab9544 0x56035cab9240 0x56035cb2d627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56050e8ec000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f46fc7b 0x7f688f47035f 0x7f688f512103 0x56035c9fa2eb 0x7f684cbd9a1c 0x7f684cc2531c 0x7f684cbdd16e 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb279ee 0x56035cababda 0x56035cb2cd00 0x56035cb279ee 0x56035cb3068c 0x7f663989600d 0x7f663989a652 0x7f66398a679b 0x7f66398a74f6 0x56035cab9c52 0x56035cb2cc25 0x7f6639894148 0x7f66398960ee 0x7f66398a80b1 0x56035c9fa2eb 0x7f6639c5b54b 0x56035cab9544 0x56035cab9240 0x56035cb2d627\n",
            "  0     200          7.40   1177.09   64.99   81.52   54.03    0.65\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56050e8ec000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f46fc7b 0x7f688f47035f 0x7f688f512103 0x56035c9fa2eb 0x7f684cbd9a1c 0x7f684cc2531c 0x7f684cbdd16e 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb279ee 0x56035cababda 0x56035cb2cd00 0x56035cb279ee 0x56035cb3068c 0x7f663989600d 0x7f663989a652 0x7f66398a679b 0x7f66398a74f6 0x56035cab9c52 0x56035cb2cc25 0x7f6639894148 0x7f66398960ee 0x7f66398a80b1 0x56035c9fa2eb 0x7f6639c5b54b 0x56035cab9544 0x56035cab9240 0x56035cb2d627\n",
            "  0     400        275.19    625.40   70.76   71.09   70.43    0.71\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x56050e8ec000 @  0x7f689191f1e7 0x7f688f41f46e 0x7f688f46fc7b 0x7f688f47035f 0x7f688f512103 0x56035c9fa2eb 0x7f684cbd9a1c 0x7f684cc2531c 0x7f684cbdd16e 0x56035cab9544 0x56035cab9240 0x56035cb2d627 0x56035cb279ee 0x56035cababda 0x56035cb2cd00 0x56035cb279ee 0x56035cb3068c 0x7f663989600d 0x7f663989a652 0x7f66398a679b 0x7f66398a74f6 0x56035cab9c52 0x56035cb2cc25 0x7f6639894148 0x7f66398960ee 0x7f66398a80b1 0x56035c9fa2eb 0x7f6639c5b54b 0x56035cab9544 0x56035cab9240 0x56035cb2d627\n",
            "  0     600         31.57    575.03   72.76   79.48   67.09    0.73\n",
            "  0     800         44.95    760.34   74.08   74.84   73.33    0.74\n",
            "  0    1000         39.14    696.27   75.68   75.32   76.05    0.76\n",
            "  1    1200         87.74    788.52   76.74   77.28   76.20    0.77\n",
            "  1    1400         73.46    793.87   79.80   80.46   79.15    0.80\n",
            "  2    1600         57.69    605.53   81.22   81.66   80.78    0.81\n",
            "  3    1800         81.90    701.72   79.96   80.75   79.19    0.80\n",
            "  3    2000         90.65    565.49   81.17   83.70   78.80    0.81\n",
            "  4    2200         87.35    504.21   80.95   83.39   78.64    0.81\n",
            "  6    2400         89.95    328.62   82.39   84.24   80.62    0.82\n",
            "  7    2600         89.72    257.78   79.08   81.13   77.13    0.79\n",
            "  8    2800        116.06    214.95   79.55   81.14   78.02    0.80\n",
            "  9    3000         79.08    169.04   80.42   82.62   78.33    0.80\n",
            " 11    3200         68.05    118.21   80.27   82.09   78.53    0.80\n",
            " 12    3400         80.97    115.24   79.70   81.66   77.83    0.80\n",
            " 13    3600         56.53     69.66   81.72   82.61   80.85    0.82\n",
            " 14    3800        150.28    113.48   81.04   82.52   79.61    0.81\n",
            " 15    4000         47.41     57.42   80.34   81.17   79.53    0.80\n",
            " 17    4200         97.50     75.39   80.92   82.49   79.42    0.81\n",
            " 18    4400         96.55     79.85   81.30   83.44   79.26    0.81\n",
            " 19    4600         85.97     76.47   79.74   82.21   77.40    0.80\n",
            " 20    4800        134.64    107.31   78.66   81.91   75.66    0.79\n",
            " 22    5000        158.40     95.72   79.80   81.58   78.10    0.80\n",
            " 23    5200         95.48     69.11   78.49   80.88   76.24    0.78\n",
            " 24    5400        135.40     93.32   80.45   81.09   79.81    0.80\n",
            " 25    5600        151.75    100.57   80.42   82.28   78.64    0.80\n",
            " 27    5800         78.41     53.50   80.02   81.04   79.03    0.80\n",
            " 28    6000        138.20     73.34   79.59   81.34   77.91    0.80\n",
            " 29    6200        125.88     61.68   80.59   82.47   78.80    0.81\n",
            " 30    6400         83.61     50.82   79.76   81.23   78.33    0.80\n",
            " 32    6600         87.96     53.90   79.34   80.58   78.14    0.79\n",
            " 33    6800        121.52     69.09   80.76   82.35   79.22    0.81\n",
            " 34    7000        184.19     97.67   80.05   82.40   77.83    0.80\n",
            " 35    7200        211.95     79.56   80.32   82.45   78.29    0.80\n",
            " 37    7400        124.56     58.74   78.92   80.71   77.21    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-05 23:25:52,028] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x563d4da1c000 @  0x7f9307478001 0x7f9304f7654f 0x7f9304fc6b58 0x7f9304fcab17 0x7f9305069203 0x563d43eef544 0x563d43eef240 0x563d43f63627 0x563d43f5dced 0x563d43ef0bda 0x563d43f5f737 0x563d43f5dced 0x563d43ef0bda 0x563d43f5f737 0x563d43f5d9ee 0x563d43e2fe2b 0x563d43f5ffe4 0x563d43f5dced 0x563d43e2fe2b 0x563d43f5ffe4 0x563d43f5d9ee 0x563d43ef148c 0x563d43ef1698 0x563d43f5ffe4 0x563d43ef0afa 0x563d43f5ec0d 0x563d43f5dced 0x563d43ef0bda 0x563d43f5ec0d 0x563d43f5dced 0x563d43ef0bda\n",
            "2665029it [03:01, 14683.84it/s]\n",
            "[2022-01-05 23:28:55,114] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-05 23:30:33,036] [INFO] Set up nlp object from config\n",
            "[2022-01-05 23:30:33,047] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-05 23:30:33,051] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558cc25d4000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516dea1e7c 0x7f516dea2aaf 0x7f516df44470 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x558bd270e9ee 0x558bd26a1bda 0x558bd2713d00 0x558bd270e9ee 0x558bd271768c 0x7f4f182c300d 0x7f4f182c9dd7 0x7f4f182cdea8 0x558bd26a0c52 0x558bd2713c25 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182d5dc3 0x558bd25e12eb 0x7f4f1869acbe 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270eced 0x558bd26a1bda\n",
            "[2022-01-05 23:30:47,046] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558cc25d4000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516de9dc7b 0x7f516de9e35f 0x7f516df40103 0x558bd25e12eb 0x7f512b607a1c 0x7f512b65331c 0x7f512b60b16e 0x558bd26a04b0 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a09bd 0x558bd28112f9 0x7f512b8a1502 0x7f512b8a480d 0x7f512b8a2275 0x7f512b8a2a1e 0x558bd26a02ed 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9dd7 0x7f4f182cd062 0x558bd26a0c52 0x558bd2713c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x558d82978000 @  0x7f517034e2a4 0x7f512b8a68bd 0x7f512b8a5a9d 0x7f512b8a2275 0x7f512b8a2a1e 0x558bd26a02ed 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9dd7 0x7f4f182cd062 0x558bd26a0c52 0x558bd2713c25 0x558bd26a1afa 0x558bd270f915 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9ab0 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558cc25d4000 @  0x7f517034d1e7 0x558bd26d1f98 0x558bd269ce27 0x7f512b8a2319 0x7f512b8a2a1e 0x558bd26a02ed 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9dd7 0x7f4f182cd062 0x558bd26a0c52 0x558bd2713c25 0x558bd26a1afa 0x558bd270f915 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9ab0 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x558f0053e000 @  0x7f517034e2a4 0x7f512b8a3fd3 0x7f512b8a5a9d 0x7f512b8a2275 0x7f512b8a2a1e 0x558bd26a02ed 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x558bd26a1afa 0x558bd2713d00 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9ab0 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x558d82978000 @  0x7f517034d1e7 0x558bd26d1f98 0x558bd269ce27 0x7f512b8a2319 0x7f512b8a2a1e 0x558bd26a02ed 0x558bd2791e1d 0x558bd2713e99 0x558bd270e9ee 0x558bd26a1bda 0x558bd2710737 0x558bd26a1afa 0x558bd2713d00 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182c9ab0 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737 0x558bd270eced 0x558bd26a1bda 0x558bd2710737\n",
            "[2022-01-05 23:30:56,403] [INFO] Finished initializing nlp object\n",
            "[2022-01-05 23:31:09,214] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558d82978000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516de9dc7b 0x7f516de9e35f 0x7f516df40103 0x558bd25e12eb 0x7f512b607a1c 0x7f512b65331c 0x7f512b60b16e 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270e9ee 0x558bd26a1bda 0x558bd2713d00 0x558bd270e9ee 0x558bd271768c 0x7f4f182c300d 0x7f4f182c7652 0x7f4f182d379b 0x7f4f182d44f6 0x558bd26a0c52 0x558bd2713c25 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182d50b1 0x558bd25e12eb 0x7f4f1868854b 0x558bd26a0544 0x558bd26a0240 0x558bd2714627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558d82978000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516de9dc7b 0x7f516de9e35f 0x7f516df40103 0x558bd25e12eb 0x7f512b607a1c 0x7f512b65331c 0x7f512b60b16e 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270e9ee 0x558bd26a1bda 0x558bd2713d00 0x558bd270e9ee 0x558bd271768c 0x7f4f182c300d 0x7f4f182c7652 0x7f4f182d379b 0x7f4f182d44f6 0x558bd26a0c52 0x558bd2713c25 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182d50b1 0x558bd25e12eb 0x7f4f1868854b 0x558bd26a0544 0x558bd26a0240 0x558bd2714627\n",
            "  0     200          5.88   1185.37   64.30   81.67   53.02    0.64\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558d82978000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516de9dc7b 0x7f516de9e35f 0x7f516df40103 0x558bd25e12eb 0x7f512b607a1c 0x7f512b65331c 0x7f512b60b16e 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270e9ee 0x558bd26a1bda 0x558bd2713d00 0x558bd270e9ee 0x558bd271768c 0x7f4f182c300d 0x7f4f182c7652 0x7f4f182d379b 0x7f4f182d44f6 0x558bd26a0c52 0x558bd2713c25 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182d50b1 0x558bd25e12eb 0x7f4f1868854b 0x558bd26a0544 0x558bd26a0240 0x558bd2714627\n",
            "  0     400        142.96    612.08   72.96   75.25   70.81    0.73\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x558d82978000 @  0x7f517034d1e7 0x7f516de4d46e 0x7f516de9dc7b 0x7f516de9e35f 0x7f516df40103 0x558bd25e12eb 0x7f512b607a1c 0x7f512b65331c 0x7f512b60b16e 0x558bd26a0544 0x558bd26a0240 0x558bd2714627 0x558bd270e9ee 0x558bd26a1bda 0x558bd2713d00 0x558bd270e9ee 0x558bd271768c 0x7f4f182c300d 0x7f4f182c7652 0x7f4f182d379b 0x7f4f182d44f6 0x558bd26a0c52 0x558bd2713c25 0x7f4f182c1148 0x7f4f182c30ee 0x7f4f182d50b1 0x558bd25e12eb 0x7f4f1868854b 0x558bd26a0544 0x558bd26a0240 0x558bd2714627\n",
            "  0     600        108.68    805.37   72.76   80.30   66.51    0.73\n",
            "  0     800         39.62    763.48   75.07   75.81   74.34    0.75\n",
            "  0    1000         56.97    753.26   76.09   75.76   76.43    0.76\n",
            "  1    1200         55.00    728.14   76.26   78.85   73.84    0.76\n",
            "  1    1400        105.26    877.61   78.06   78.64   77.48    0.78\n",
            "  2    1600         60.44    637.68   80.26   79.68   80.85    0.80\n",
            "  3    1800         94.45    823.23   79.66   81.53   77.87    0.80\n",
            "  3    2000         82.03    556.02   79.98   81.58   78.45    0.80\n",
            "  4    2200         85.89    531.76   80.99   82.79   79.26    0.81\n",
            "  6    2400        102.65    401.28   81.03   83.84   78.41    0.81\n",
            "  7    2600         63.49    240.23   78.46   80.59   76.43    0.78\n",
            "  8    2800        136.61    181.78   80.01   81.25   78.80    0.80\n",
            "  9    3000        103.17    172.28   81.18   82.31   80.08    0.81\n",
            " 11    3200         37.88     64.96   80.52   82.57   78.57    0.81\n",
            " 12    3400        163.08    153.40   78.51   79.90   77.17    0.79\n",
            " 13    3600         82.68    106.60   79.18   82.67   75.97    0.79\n",
            " 14    3800         57.65     66.29   79.62   80.83   78.45    0.80\n",
            " 15    4000         60.11     84.86   79.97   81.63   78.37    0.80\n",
            " 17    4200         76.61     65.16   80.14   83.04   77.44    0.80\n",
            " 18    4400        110.70    104.81   79.90   82.88   77.13    0.80\n",
            " 19    4600        112.47     91.73   79.89   80.89   78.91    0.80\n",
            " 20    4800        125.39     82.12   81.42   82.81   80.08    0.81\n",
            " 22    5000        141.32    114.68   80.23   82.74   77.87    0.80\n",
            " 23    5200        229.38    160.65   78.93   81.33   76.67    0.79\n",
            " 24    5400        165.21    147.15   79.95   80.96   78.95    0.80\n",
            " 25    5600        247.54    136.98   78.88   80.37   77.44    0.79\n",
            " 27    5800        129.43     82.93   77.44   79.32   75.66    0.77\n",
            " 28    6000        197.31    111.17   80.51   81.99   79.07    0.81\n",
            " 29    6200        157.14    105.11   80.35   81.15   79.57    0.80\n",
            " 30    6400         69.47     62.76   79.04   82.06   76.24    0.79\n",
            " 32    6600        139.50     89.12   79.22   83.32   75.50    0.79\n",
            " 33    6800         52.15     35.34   80.43   82.42   78.53    0.80\n",
            " 34    7000        208.59     95.79   79.90   82.01   77.91    0.80\n",
            " 35    7200        111.70     76.50   78.75   80.91   76.71    0.79\n",
            " 37    7400         93.56     54.93   81.13   83.01   79.34    0.81\n",
            " 38    7600         93.16     60.03   80.34   81.52   79.19    0.80\n",
            " 39    7800         73.90     51.92   79.53   81.86   77.33    0.80\n",
            " 40    8000         56.57     29.67   81.14   82.19   80.12    0.81\n",
            " 42    8200        158.54     58.78   79.76   81.04   78.53    0.80\n",
            " 43    8400         87.58     42.05   80.35   82.96   77.91    0.80\n",
            " 44    8600        108.88     57.17   80.51   82.33   78.76    0.81\n",
            " 45    8800        156.31     82.91   79.51   80.15   78.88    0.80\n",
            " 46    9000        220.27     99.00   79.45   81.44   77.56    0.79\n",
            " 48    9200        189.99     55.78   78.83   81.24   76.55    0.79\n",
            " 49    9400         39.44     22.00   77.39   79.24   75.62    0.77\n",
            " 50    9600        134.17     50.69   78.77   81.42   76.28    0.79\n",
            " 51    9800        125.24     62.11   78.09   80.16   76.12    0.78\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 00:47:17,345] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555f319a0000 @  0x7f4b3e721001 0x7f4b3c21f54f 0x7f4b3c26fb58 0x7f4b3c273b17 0x7f4b3c312203 0x555f27f8e544 0x555f27f8e240 0x555f28002627 0x555f27ffcced 0x555f27f8fbda 0x555f27ffe737 0x555f27ffcced 0x555f27f8fbda 0x555f27ffe737 0x555f27ffc9ee 0x555f27ecee2b 0x555f27ffefe4 0x555f27ffcced 0x555f27ecee2b 0x555f27ffefe4 0x555f27ffc9ee 0x555f27f9048c 0x555f27f90698 0x555f27ffefe4 0x555f27f8fafa 0x555f27ffdc0d 0x555f27ffcced 0x555f27f8fbda 0x555f27ffdc0d 0x555f27ffcced 0x555f27f8fbda\n",
            "2665029it [03:25, 12995.18it/s]\n",
            "[2022-01-06 00:50:44,035] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-06 00:52:24,573] [INFO] Set up nlp object from config\n",
            "[2022-01-06 00:52:24,582] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-06 00:52:24,586] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564f258d0000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cee7c 0x7fbee95cfaaf 0x7fbee9671470 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x564e36a2c9ee 0x564e369bfbda 0x564e36a31d00 0x564e36a2c9ee 0x564e36a3568c 0x7fbc939ef00d 0x7fbc939f5dd7 0x7fbc939f9ea8 0x564e369bec52 0x564e36a31c25 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc93a01dc3 0x564e368ff2eb 0x7fbc93dc6cbe 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2cced 0x564e369bfbda\n",
            "[2022-01-06 00:52:38,947] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564f258d0000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cac7b 0x7fbee95cb35f 0x7fbee966d103 0x564e368ff2eb 0x7fbea6d34a1c 0x7fbea6d8031c 0x7fbea6d3816e 0x564e369be4b0 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369be9bd 0x564e36b2f2f9 0x7fbea6fce502 0x7fbea6fd180d 0x7fbea6fcf275 0x7fbea6fcfa1e 0x564e369be2ed 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5dd7 0x7fbc939f9062 0x564e369bec52 0x564e36a31c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x564fe7bc4000 @  0x7fbeeba7b2a4 0x7fbea6fd38bd 0x7fbea6fd2a9d 0x7fbea6fcf275 0x7fbea6fcfa1e 0x564e369be2ed 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5dd7 0x7fbc939f9062 0x564e369bec52 0x564e36a31c25 0x564e369bfafa 0x564e36a2d915 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5ab0 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564f258d0000 @  0x7fbeeba7a1e7 0x564e369eff98 0x564e369bae27 0x7fbea6fcf319 0x7fbea6fcfa1e 0x564e369be2ed 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5dd7 0x7fbc939f9062 0x564e369bec52 0x564e36a31c25 0x564e369bfafa 0x564e36a2d915 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5ab0 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x56516578a000 @  0x7fbeeba7b2a4 0x7fbea6fd0fd3 0x7fbea6fd2a9d 0x7fbea6fcf275 0x7fbea6fcfa1e 0x564e369be2ed 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x564e369bfafa 0x564e36a31d00 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5ab0 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x564fe7bc4000 @  0x7fbeeba7a1e7 0x564e369eff98 0x564e369bae27 0x7fbea6fcf319 0x7fbea6fcfa1e 0x564e369be2ed 0x564e36aafe1d 0x564e36a31e99 0x564e36a2c9ee 0x564e369bfbda 0x564e36a2e737 0x564e369bfafa 0x564e36a31d00 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc939f5ab0 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737 0x564e36a2cced 0x564e369bfbda 0x564e36a2e737\n",
            "[2022-01-06 00:52:48,213] [INFO] Finished initializing nlp object\n",
            "[2022-01-06 00:53:01,106] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564fe7bc4000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cac7b 0x7fbee95cb35f 0x7fbee966d103 0x564e368ff2eb 0x7fbea6d34a1c 0x7fbea6d8031c 0x7fbea6d3816e 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2c9ee 0x564e369bfbda 0x564e36a31d00 0x564e36a2c9ee 0x564e36a3568c 0x7fbc939ef00d 0x7fbc939f3652 0x7fbc939ff79b 0x7fbc93a004f6 0x564e369bec52 0x564e36a31c25 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc93a010b1 0x564e368ff2eb 0x7fbc93db454b 0x564e369be544 0x564e369be240 0x564e36a32627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564fe7bc4000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cac7b 0x7fbee95cb35f 0x7fbee966d103 0x564e368ff2eb 0x7fbea6d34a1c 0x7fbea6d8031c 0x7fbea6d3816e 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2c9ee 0x564e369bfbda 0x564e36a31d00 0x564e36a2c9ee 0x564e36a3568c 0x7fbc939ef00d 0x7fbc939f3652 0x7fbc939ff79b 0x7fbc93a004f6 0x564e369bec52 0x564e36a31c25 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc93a010b1 0x564e368ff2eb 0x7fbc93db454b 0x564e369be544 0x564e369be240 0x564e36a32627\n",
            "  0     200          5.42   1175.93   69.04   80.85   60.23    0.69\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564fe7bc4000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cac7b 0x7fbee95cb35f 0x7fbee966d103 0x564e368ff2eb 0x7fbea6d34a1c 0x7fbea6d8031c 0x7fbea6d3816e 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2c9ee 0x564e369bfbda 0x564e36a31d00 0x564e36a2c9ee 0x564e36a3568c 0x7fbc939ef00d 0x7fbc939f3652 0x7fbc939ff79b 0x7fbc93a004f6 0x564e369bec52 0x564e36a31c25 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc93a010b1 0x564e368ff2eb 0x7fbc93db454b 0x564e369be544 0x564e369be240 0x564e36a32627\n",
            "  0     400         30.19    560.49   70.22   71.09   69.38    0.70\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x564fe7bc4000 @  0x7fbeeba7a1e7 0x7fbee957a46e 0x7fbee95cac7b 0x7fbee95cb35f 0x7fbee966d103 0x564e368ff2eb 0x7fbea6d34a1c 0x7fbea6d8031c 0x7fbea6d3816e 0x564e369be544 0x564e369be240 0x564e36a32627 0x564e36a2c9ee 0x564e369bfbda 0x564e36a31d00 0x564e36a2c9ee 0x564e36a3568c 0x7fbc939ef00d 0x7fbc939f3652 0x7fbc939ff79b 0x7fbc93a004f6 0x564e369bec52 0x564e36a31c25 0x7fbc939ed148 0x7fbc939ef0ee 0x7fbc93a010b1 0x564e368ff2eb 0x7fbc93db454b 0x564e369be544 0x564e369be240 0x564e36a32627\n",
            "  0     600         87.07    612.52   73.14   77.77   69.03    0.73\n",
            "  0     800         41.48    758.07   74.53   74.72   74.34    0.75\n",
            "  0    1000        211.78    810.93   76.40   76.14   76.67    0.76\n",
            "  1    1200         61.19    709.79   78.17   78.68   77.67    0.78\n",
            "  1    1400         77.09    735.67   79.45   79.63   79.26    0.79\n",
            "  2    1600         71.96    661.26   77.74   76.93   78.57    0.78\n",
            "  3    1800        103.07    773.18   78.48   83.10   74.34    0.78\n",
            "  3    2000        116.60    598.73   80.72   83.87   77.79    0.81\n",
            "  4    2200        107.94    535.31   79.25   81.75   76.90    0.79\n",
            "  6    2400        138.49    341.37   80.14   82.69   77.75    0.80\n",
            "  7    2600        145.22    264.71   80.15   81.72   78.64    0.80\n",
            "  8    2800        105.84    276.21   80.44   82.03   78.91    0.80\n",
            "  9    3000         86.26    162.09   80.66   81.30   80.04    0.81\n",
            " 11    3200         70.87    125.46   80.58   80.82   80.35    0.81\n",
            " 12    3400         93.05    110.33   80.97   81.89   80.08    0.81\n",
            " 13    3600         75.02    101.79   79.05   80.98   77.21    0.79\n",
            " 14    3800         72.54     85.84   79.08   79.12   79.03    0.79\n",
            " 15    4000        223.70    120.07   80.70   82.91   78.60    0.81\n",
            " 17    4200         88.74     91.02   78.80   80.45   77.21    0.79\n",
            " 18    4400         78.36     96.67   79.59   81.34   77.91    0.80\n",
            " 19    4600        143.22    127.44   80.25   80.42   80.08    0.80\n",
            " 20    4800         55.25     49.51   80.80   81.26   80.35    0.81\n",
            " 22    5000         94.30     80.74   79.54   80.21   78.88    0.80\n",
            " 23    5200         65.67     58.40   80.82   81.62   80.04    0.81\n",
            " 24    5400        298.31    131.46   80.91   82.34   79.53    0.81\n",
            " 25    5600        104.25     79.19   81.01   82.20   79.84    0.81\n",
            " 27    5800        196.90     63.16   80.65   82.92   78.49    0.81\n",
            " 28    6000        157.97    103.07   80.85   82.38   79.38    0.81\n",
            " 29    6200        150.92     62.96   80.88   82.19   79.61    0.81\n",
            " 30    6400        125.46     74.79   77.48   79.74   75.35    0.77\n",
            " 32    6600        142.16    108.38   78.27   82.73   74.26    0.78\n",
            " 33    6800        366.07    154.51   81.49   83.55   79.53    0.81\n",
            " 34    7000        160.28    106.27   79.64   81.28   78.06    0.80\n",
            " 35    7200        110.34     49.24   76.49   79.42   73.76    0.76\n",
            " 37    7400         95.25     50.91   79.47   81.90   77.17    0.79\n",
            " 38    7600        134.10     36.07   78.94   79.67   78.22    0.79\n",
            " 39    7800         88.33     51.79   80.61   81.91   79.34    0.81\n",
            " 40    8000         88.91     43.83   80.25   81.39   79.15    0.80\n",
            " 42    8200        119.91     71.58   77.76   79.86   75.78    0.78\n",
            " 43    8400        132.18     72.51   79.05   80.10   78.02    0.79\n",
            " 44    8600        253.97    110.46   80.27   81.31   79.26    0.80\n",
            " 45    8800        194.17    114.98   78.75   80.82   76.78    0.79\n",
            " 46    9000        303.33     82.91   79.61   80.94   78.33    0.80\n",
            " 48    9200        121.21     65.41   79.37   81.36   77.48    0.79\n",
            " 49    9400        153.71     79.44   79.33   79.91   78.76    0.79\n",
            " 50    9600         86.93     50.68   80.15   82.23   78.18    0.80\n",
            " 51    9800         61.36     35.54   80.56   81.65   79.50    0.81\n",
            " 53   10000         36.92     18.30   79.38   82.70   76.32    0.79\n",
            " 54   10200         83.36     33.43   78.73   80.96   76.63    0.79\n",
            " 55   10400         28.81     14.25   80.98   83.02   79.03    0.81\n",
            " 56   10600         16.88      6.27   80.01   81.93   78.18    0.80\n",
            " 58   10800         79.96     38.67   80.57   84.11   77.33    0.81\n",
            " 59   11000        193.96     54.20   77.28   79.53   75.16    0.77\n",
            " 60   11200        129.00     56.96   79.92   81.12   78.76    0.80\n",
            " 61   11400        263.17    136.33   79.54   81.24   77.91    0.80\n",
            " 63   11600        134.02     63.54   77.48   80.12   75.00    0.77\n",
            " 64   11800        263.93     60.92   80.17   81.92   78.49    0.80\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-3.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 02:18:05,371] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x562a838da000 @  0x7fcb40c88001 0x7fcb3e78654f 0x7fcb3e7d6b58 0x7fcb3e7dab17 0x7fcb3e879203 0x562a78f1f544 0x562a78f1f240 0x562a78f93627 0x562a78f8dced 0x562a78f20bda 0x562a78f8f737 0x562a78f8dced 0x562a78f20bda 0x562a78f8f737 0x562a78f8d9ee 0x562a78e5fe2b 0x562a78f8ffe4 0x562a78f8dced 0x562a78e5fe2b 0x562a78f8ffe4 0x562a78f8d9ee 0x562a78f2148c 0x562a78f21698 0x562a78f8ffe4 0x562a78f20afa 0x562a78f8ec0d 0x562a78f8dced 0x562a78f20bda 0x562a78f8ec0d 0x562a78f8dced 0x562a78f20bda\n",
            "2665029it [03:06, 14294.49it/s]\n",
            "[2022-01-06 02:21:13,444] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-06 02:22:52,655] [INFO] Set up nlp object from config\n",
            "[2022-01-06 02:22:52,665] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-06 02:22:52,669] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555b343e0000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e074ce7c 0x7f29e074daaf 0x7f29e07ef470 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67dd00 0x555a5d6789ee 0x555a5d68168c 0x7f278ab5f00d 0x7f278ab65dd7 0x7f278ab69ea8 0x555a5d60ac52 0x555a5d67dc25 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab71dc3 0x555a5d54b2eb 0x7f278af36cbe 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d678ced 0x555a5d60bbda\n",
            "[2022-01-06 02:23:07,035] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555b343e0000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e0748c7b 0x7f29e074935f 0x7f29e07eb103 0x555a5d54b2eb 0x7f299deb2a1c 0x7f299defe31c 0x7f299deb616e 0x555a5d60a4b0 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60a9bd 0x555a5d77b2f9 0x7f299e14c502 0x7f299e14f80d 0x7f299e14d275 0x7f299e14da1e 0x555a5d60a2ed 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65dd7 0x7f278ab69062 0x555a5d60ac52 0x555a5d67dc25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x555c0de14000 @  0x7f29e2bf92a4 0x7f299e1518bd 0x7f299e150a9d 0x7f299e14d275 0x7f299e14da1e 0x555a5d60a2ed 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65dd7 0x7f278ab69062 0x555a5d60ac52 0x555a5d67dc25 0x555a5d60bafa 0x555a5d679915 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65ab0 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555b343e0000 @  0x7f29e2bf81e7 0x555a5d63bf98 0x555a5d606e27 0x7f299e14d319 0x7f299e14da1e 0x555a5d60a2ed 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65dd7 0x7f278ab69062 0x555a5d60ac52 0x555a5d67dc25 0x555a5d60bafa 0x555a5d679915 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65ab0 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x555d8b9da000 @  0x7f29e2bf92a4 0x7f299e14efd3 0x7f299e150a9d 0x7f299e14d275 0x7f299e14da1e 0x555a5d60a2ed 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x555a5d60bafa 0x555a5d67dd00 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65ab0 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x555c0de14000 @  0x7f29e2bf81e7 0x555a5d63bf98 0x555a5d606e27 0x7f299e14d319 0x7f299e14da1e 0x555a5d60a2ed 0x555a5d6fbe1d 0x555a5d67de99 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67a737 0x555a5d60bafa 0x555a5d67dd00 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab65ab0 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737 0x555a5d678ced 0x555a5d60bbda 0x555a5d67a737\n",
            "[2022-01-06 02:23:16,259] [INFO] Finished initializing nlp object\n",
            "[2022-01-06 02:23:29,312] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555c0de14000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e0748c7b 0x7f29e074935f 0x7f29e07eb103 0x555a5d54b2eb 0x7f299deb2a1c 0x7f299defe31c 0x7f299deb616e 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67dd00 0x555a5d6789ee 0x555a5d68168c 0x7f278ab5f00d 0x7f278ab63652 0x7f278ab6f79b 0x7f278ab704f6 0x555a5d60ac52 0x555a5d67dc25 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab710b1 0x555a5d54b2eb 0x7f278af2454b 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555c0de14000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e0748c7b 0x7f29e074935f 0x7f29e07eb103 0x555a5d54b2eb 0x7f299deb2a1c 0x7f299defe31c 0x7f299deb616e 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67dd00 0x555a5d6789ee 0x555a5d68168c 0x7f278ab5f00d 0x7f278ab63652 0x7f278ab6f79b 0x7f278ab704f6 0x555a5d60ac52 0x555a5d67dc25 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab710b1 0x555a5d54b2eb 0x7f278af2454b 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627\n",
            "  0     200          8.63   1193.92   59.61   78.65   47.98    0.60\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555c0de14000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e0748c7b 0x7f29e074935f 0x7f29e07eb103 0x555a5d54b2eb 0x7f299deb2a1c 0x7f299defe31c 0x7f299deb616e 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67dd00 0x555a5d6789ee 0x555a5d68168c 0x7f278ab5f00d 0x7f278ab63652 0x7f278ab6f79b 0x7f278ab704f6 0x555a5d60ac52 0x555a5d67dc25 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab710b1 0x555a5d54b2eb 0x7f278af2454b 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627\n",
            "  0     400         55.27    558.52   72.95   76.30   69.88    0.73\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x555c0de14000 @  0x7f29e2bf81e7 0x7f29e06f846e 0x7f29e0748c7b 0x7f29e074935f 0x7f29e07eb103 0x555a5d54b2eb 0x7f299deb2a1c 0x7f299defe31c 0x7f299deb616e 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627 0x555a5d6789ee 0x555a5d60bbda 0x555a5d67dd00 0x555a5d6789ee 0x555a5d68168c 0x7f278ab5f00d 0x7f278ab63652 0x7f278ab6f79b 0x7f278ab704f6 0x555a5d60ac52 0x555a5d67dc25 0x7f278ab5d148 0x7f278ab5f0ee 0x7f278ab710b1 0x555a5d54b2eb 0x7f278af2454b 0x555a5d60a544 0x555a5d60a240 0x555a5d67e627\n",
            "  0     600         71.77    642.84   72.98   79.55   67.40    0.73\n",
            "  0     800         92.98    738.14   76.50   77.90   75.16    0.77\n",
            "  0    1000         46.58    687.79   75.51   75.25   75.78    0.76\n",
            "  1    1200         65.73    747.72   79.14   80.62   77.71    0.79\n",
            "  1    1400         59.61    752.20   79.13   80.42   77.87    0.79\n",
            "  2    1600         63.86    593.63   80.63   81.32   79.96    0.81\n",
            "  3    1800        114.75    701.77   79.76   81.96   77.67    0.80\n",
            "  3    2000         98.29    613.08   80.44   83.73   77.40    0.80\n",
            "  4    2200        106.38    596.93   81.89   85.46   78.60    0.82\n",
            "  6    2400        101.20    400.11   81.74   84.43   79.22    0.82\n",
            "  7    2600         83.03    276.45   80.72   81.95   79.53    0.81\n",
            "  8    2800        140.53    225.94   81.32   83.24   79.50    0.81\n",
            "  9    3000         85.36    162.27   80.97   81.48   80.47    0.81\n",
            " 11    3200         85.42    120.36   81.35   83.90   78.95    0.81\n",
            " 12    3400         92.18    131.13   81.14   82.72   79.61    0.81\n",
            " 13    3600         89.98     86.53   80.14   82.89   77.56    0.80\n",
            " 14    3800        109.30    139.63   80.81   82.26   79.42    0.81\n",
            " 15    4000         93.23    107.07   80.22   82.71   77.87    0.80\n",
            " 17    4200         73.34     75.31   80.04   80.19   79.88    0.80\n",
            " 18    4400         92.38     87.95   80.01   83.06   77.17    0.80\n",
            " 19    4600        253.75    127.29   78.89   81.29   76.63    0.79\n",
            " 20    4800        106.46    105.81   81.22   82.44   80.04    0.81\n",
            " 22    5000        114.20     78.94   80.26   82.03   78.57    0.80\n",
            " 23    5200        157.51    120.38   81.08   82.39   79.81    0.81\n",
            " 24    5400        156.59    101.23   80.97   83.88   78.26    0.81\n",
            " 25    5600         51.60     45.47   81.60   82.78   80.47    0.82\n",
            " 27    5800        316.90     89.51   79.49   80.36   78.64    0.79\n",
            " 28    6000        186.44    114.70   79.12   81.33   77.02    0.79\n",
            " 29    6200        122.06     99.22   79.52   80.10   78.95    0.80\n",
            " 30    6400        127.98     88.86   79.57   81.31   77.91    0.80\n",
            " 32    6600         86.55     73.15   77.26   79.23   75.39    0.77\n",
            " 33    6800        151.09     65.48   80.53   81.48   79.61    0.81\n",
            " 34    7000         87.48     61.21   79.69   80.88   78.53    0.80\n",
            " 35    7200         90.36     53.86   78.12   80.85   75.58    0.78\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-3.neg_sampling-10/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 03:26:59,273] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5653aa9ee000 @  0x7fd118a1c001 0x7fd11651a54f 0x7fd11656ab58 0x7fd11656eb17 0x7fd11660d203 0x56539faf6544 0x56539faf6240 0x56539fb6a627 0x56539fb64ced 0x56539faf7bda 0x56539fb66737 0x56539fb64ced 0x56539faf7bda 0x56539fb66737 0x56539fb649ee 0x56539fa36e2b 0x56539fb66fe4 0x56539fb64ced 0x56539fa36e2b 0x56539fb66fe4 0x56539fb649ee 0x56539faf848c 0x56539faf8698 0x56539fb66fe4 0x56539faf7afa 0x56539fb65c0d 0x56539fb64ced 0x56539faf7bda 0x56539fb65c0d 0x56539fb64ced 0x56539faf7bda\n",
            "2665029it [03:06, 14273.72it/s]\n",
            "[2022-01-06 03:30:07,628] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-06 03:31:48,842] [INFO] Set up nlp object from config\n",
            "[2022-01-06 03:31:48,852] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-06 03:31:48,856] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5610d8a8a000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd34e7c 0x7f10edd35aaf 0x7f10eddd7470 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x560fea1b29ee 0x560fea145bda 0x560fea1b7d00 0x560fea1b29ee 0x560fea1bb68c 0x7f0e9815600d 0x7f0e9815cdd7 0x7f0e98160ea8 0x560fea144c52 0x560fea1b7c25 0x7f0e98154148 0x7f0e981560ee 0x7f0e98168dc3 0x560fea0852eb 0x7f0e9852dcbe 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b2ced 0x560fea145bda\n",
            "[2022-01-06 03:32:03,559] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5610d8a8a000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd30c7b 0x7f10edd3135f 0x7f10eddd3103 0x560fea0852eb 0x7f10ab49aa1c 0x7f10ab4e631c 0x7f10ab49e16e 0x560fea1444b0 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea1449bd 0x560fea2b52f9 0x7f10ab734502 0x7f10ab73780d 0x7f10ab735275 0x7f10ab735a1e 0x560fea1442ed 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cdd7 0x7f0e98160062 0x560fea144c52 0x560fea1b7c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x561199f98000 @  0x7f10f01e12a4 0x7f10ab7398bd 0x7f10ab738a9d 0x7f10ab735275 0x7f10ab735a1e 0x560fea1442ed 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cdd7 0x7f0e98160062 0x560fea144c52 0x560fea1b7c25 0x560fea145afa 0x560fea1b3915 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cab0 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5610d8a8a000 @  0x7f10f01e01e7 0x560fea175f98 0x560fea140e27 0x7f10ab735319 0x7f10ab735a1e 0x560fea1442ed 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cdd7 0x7f0e98160062 0x560fea144c52 0x560fea1b7c25 0x560fea145afa 0x560fea1b3915 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cab0 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x561317b5e000 @  0x7f10f01e12a4 0x7f10ab736fd3 0x7f10ab738a9d 0x7f10ab735275 0x7f10ab735a1e 0x560fea1442ed 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x560fea145afa 0x560fea1b7d00 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cab0 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x561199f98000 @  0x7f10f01e01e7 0x560fea175f98 0x560fea140e27 0x7f10ab735319 0x7f10ab735a1e 0x560fea1442ed 0x560fea235e1d 0x560fea1b7e99 0x560fea1b29ee 0x560fea145bda 0x560fea1b4737 0x560fea145afa 0x560fea1b7d00 0x7f0e98154148 0x7f0e981560ee 0x7f0e9815cab0 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737 0x560fea1b2ced 0x560fea145bda 0x560fea1b4737\n",
            "[2022-01-06 03:32:12,894] [INFO] Finished initializing nlp object\n",
            "[2022-01-06 03:32:25,972] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x561199f98000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd30c7b 0x7f10edd3135f 0x7f10eddd3103 0x560fea0852eb 0x7f10ab49aa1c 0x7f10ab4e631c 0x7f10ab49e16e 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b29ee 0x560fea145bda 0x560fea1b7d00 0x560fea1b29ee 0x560fea1bb68c 0x7f0e9815600d 0x7f0e9815a652 0x7f0e9816679b 0x7f0e981674f6 0x560fea144c52 0x560fea1b7c25 0x7f0e98154148 0x7f0e981560ee 0x7f0e981680b1 0x560fea0852eb 0x7f0e9851b54b 0x560fea144544 0x560fea144240 0x560fea1b8627\n",
            "  0       0          0.00     46.94    0.00    0.00    0.00    0.00\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x561199f98000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd30c7b 0x7f10edd3135f 0x7f10eddd3103 0x560fea0852eb 0x7f10ab49aa1c 0x7f10ab4e631c 0x7f10ab49e16e 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b29ee 0x560fea145bda 0x560fea1b7d00 0x560fea1b29ee 0x560fea1bb68c 0x7f0e9815600d 0x7f0e9815a652 0x7f0e9816679b 0x7f0e981674f6 0x560fea144c52 0x560fea1b7c25 0x7f0e98154148 0x7f0e981560ee 0x7f0e981680b1 0x560fea0852eb 0x7f0e9851b54b 0x560fea144544 0x560fea144240 0x560fea1b8627\n",
            "  0     200          6.41   1156.78   61.69   74.81   52.48    0.62\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x561199f98000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd30c7b 0x7f10edd3135f 0x7f10eddd3103 0x560fea0852eb 0x7f10ab49aa1c 0x7f10ab4e631c 0x7f10ab49e16e 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b29ee 0x560fea145bda 0x560fea1b7d00 0x560fea1b29ee 0x560fea1bb68c 0x7f0e9815600d 0x7f0e9815a652 0x7f0e9816679b 0x7f0e981674f6 0x560fea144c52 0x560fea1b7c25 0x7f0e98154148 0x7f0e981560ee 0x7f0e981680b1 0x560fea0852eb 0x7f0e9851b54b 0x560fea144544 0x560fea144240 0x560fea1b8627\n",
            "  0     400         70.81    546.29   71.20   71.40   71.01    0.71\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x561199f98000 @  0x7f10f01e01e7 0x7f10edce046e 0x7f10edd30c7b 0x7f10edd3135f 0x7f10eddd3103 0x560fea0852eb 0x7f10ab49aa1c 0x7f10ab4e631c 0x7f10ab49e16e 0x560fea144544 0x560fea144240 0x560fea1b8627 0x560fea1b29ee 0x560fea145bda 0x560fea1b7d00 0x560fea1b29ee 0x560fea1bb68c 0x7f0e9815600d 0x7f0e9815a652 0x7f0e9816679b 0x7f0e981674f6 0x560fea144c52 0x560fea1b7c25 0x7f0e98154148 0x7f0e981560ee 0x7f0e981680b1 0x560fea0852eb 0x7f0e9851b54b 0x560fea144544 0x560fea144240 0x560fea1b8627\n",
            "  0     600         45.37    602.49   72.36   79.84   66.16    0.72\n",
            "  0     800         42.45    779.40   74.84   76.16   73.57    0.75\n",
            "  0    1000         45.17    700.90   71.72   73.01   70.47    0.72\n",
            "  1    1200         61.44    746.14   75.98   77.20   74.81    0.76\n",
            "  1    1400         70.39    775.23   79.68   80.18   79.19    0.80\n",
            "  2    1600         64.61    621.21   81.08   81.43   80.74    0.81\n",
            "  3    1800        104.66    733.14   80.36   81.95   78.84    0.80\n",
            "  3    2000         77.16    537.07   81.07   83.05   79.19    0.81\n",
            "  4    2200        102.39    531.39   80.71   83.15   78.41    0.81\n",
            "  6    2400        109.47    407.99   80.88   83.17   78.72    0.81\n",
            "  7    2600        107.15    308.31   80.93   82.46   79.46    0.81\n",
            "  8    2800         65.37    188.43   80.65   82.72   78.68    0.81\n",
            "  9    3000         63.63    127.81   79.97   81.88   78.14    0.80\n",
            " 11    3200        124.44    158.90   79.69   83.06   76.59    0.80\n",
            " 12    3400        108.37    123.97   80.84   82.43   79.30    0.81\n",
            " 13    3600         70.98     79.18   79.51   83.16   76.16    0.80\n",
            " 14    3800         93.55    116.17   80.29   81.34   79.26    0.80\n",
            " 15    4000         53.66     63.27   78.96   80.13   77.83    0.79\n",
            " 17    4200        112.73     67.16   79.62   79.51   79.73    0.80\n",
            " 18    4400        171.55    129.71   80.46   81.98   78.99    0.80\n",
            " 19    4600        114.64     92.04   79.99   82.41   77.71    0.80\n",
            " 20    4800        215.31    101.45   78.12   80.48   75.89    0.78\n",
            " 22    5000        192.18    114.74   78.36   78.98   77.75    0.78\n",
            " 23    5200         97.28     70.58   79.16   79.93   78.41    0.79\n",
            " 24    5400         70.26     58.40   79.72   82.80   76.86    0.80\n",
            " 25    5600         99.34     68.57   80.13   81.85   78.49    0.80\n",
            " 27    5800         61.98     40.24   79.20   81.33   77.17    0.79\n",
            " 28    6000        193.46    124.33   77.93   78.90   76.98    0.78\n",
            " 29    6200        101.55     72.55   78.49   82.10   75.19    0.78\n",
            " 30    6400        214.75    103.47   79.52   82.32   76.90    0.80\n",
            " 32    6600        108.73     62.71   79.38   81.99   76.94    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-5.neg_sampling-15/model-last\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 04:26:55,027] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5559a1de6000 @  0x7feeebc11001 0x7feee970f54f 0x7feee975fb58 0x7feee9763b17 0x7feee9802203 0x555997cdf544 0x555997cdf240 0x555997d53627 0x555997d4dced 0x555997ce0bda 0x555997d4f737 0x555997d4dced 0x555997ce0bda 0x555997d4f737 0x555997d4d9ee 0x555997c1fe2b 0x555997d4ffe4 0x555997d4dced 0x555997c1fe2b 0x555997d4ffe4 0x555997d4d9ee 0x555997ce148c 0x555997ce1698 0x555997d4ffe4 0x555997ce0afa 0x555997d4ec0d 0x555997d4dced 0x555997ce0bda 0x555997d4ec0d 0x555997d4dced 0x555997ce0bda\n",
            "2665029it [03:15, 13605.80it/s]\n",
            "[2022-01-06 04:30:12,528] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-06 04:31:48,587] [INFO] Set up nlp object from config\n",
            "[2022-01-06 04:31:48,597] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-01-06 04:31:48,601] [INFO] Created vocabulary\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55ee70db2000 @  0x7f0566a681e7 0x7f056456846e 0x7f05645bce7c 0x7f05645bdaaf 0x7f056465f470 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c78d00 0x55ed99c739ee 0x55ed99c7c68c 0x7f030e9d400d 0x7f030e9dadd7 0x7f030e9deea8 0x55ed99c05c52 0x55ed99c78c25 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9e6dc3 0x55ed99b462eb 0x7f030edabcbe 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c73ced 0x55ed99c06bda\n",
            "[2022-01-06 04:32:02,906] [INFO] Added vectors: /tmp/FastText/\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55ee70db2000 @  0x7f0566a681e7 0x7f056456846e 0x7f05645b8c7b 0x7f05645b935f 0x7f056465b103 0x55ed99b462eb 0x7f0521d22a1c 0x7f0521d6e31c 0x7f0521d2616e 0x55ed99c054b0 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c059bd 0x55ed99d762f9 0x7f0521fbc502 0x7f0521fbf80d 0x7f0521fbd275 0x7f0521fbda1e 0x55ed99c052ed 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9dadd7 0x7f030e9de062 0x55ed99c05c52 0x55ed99c78c25\n",
            "tcmalloc: large alloc 6396076032 bytes == 0x55ef49aec000 @  0x7f0566a692a4 0x7f0521fc18bd 0x7f0521fc0a9d 0x7f0521fbd275 0x7f0521fbda1e 0x55ed99c052ed 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9dadd7 0x7f030e9de062 0x55ed99c05c52 0x55ed99c78c25 0x55ed99c06afa 0x55ed99c74915 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9daab0 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55ee70db2000 @  0x7f0566a681e7 0x55ed99c36f98 0x55ed99c01e27 0x7f0521fbd319 0x7f0521fbda1e 0x55ed99c052ed 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9dadd7 0x7f030e9de062 0x55ed99c05c52 0x55ed99c78c25 0x55ed99c06afa 0x55ed99c74915 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9daab0 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737\n",
            "tcmalloc: large alloc 6470434816 bytes == 0x55f0c76b2000 @  0x7f0566a692a4 0x7f0521fbefd3 0x7f0521fc0a9d 0x7f0521fbd275 0x7f0521fbda1e 0x55ed99c052ed 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x55ed99c06afa 0x55ed99c78d00 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9daab0 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737\n",
            "tcmalloc: large alloc 3235217408 bytes == 0x55ef49aec000 @  0x7f0566a681e7 0x55ed99c36f98 0x55ed99c01e27 0x7f0521fbd319 0x7f0521fbda1e 0x55ed99c052ed 0x55ed99cf6e1d 0x55ed99c78e99 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c75737 0x55ed99c06afa 0x55ed99c78d00 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9daab0 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737 0x55ed99c73ced 0x55ed99c06bda 0x55ed99c75737\n",
            "[2022-01-06 04:32:12,035] [INFO] Finished initializing nlp object\n",
            "[2022-01-06 04:32:24,978] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55ef49aec000 @  0x7f0566a681e7 0x7f056456846e 0x7f05645b8c7b 0x7f05645b935f 0x7f056465b103 0x55ed99b462eb 0x7f0521d22a1c 0x7f0521d6e31c 0x7f0521d2616e 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627 0x55ed99c739ee 0x55ed99c06bda 0x55ed99c78d00 0x55ed99c739ee 0x55ed99c7c68c 0x7f030e9d400d 0x7f030e9d8652 0x7f030e9e479b 0x7f030e9e54f6 0x55ed99c05c52 0x55ed99c78c25 0x7f030e9d2148 0x7f030e9d40ee 0x7f030e9e60b1 0x55ed99b462eb 0x7f030ed9954b 0x55ed99c05544 0x55ed99c05240 0x55ed99c79627\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "Error([('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/strings.json',\n",
            "'/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/strings.json',\n",
            "'[Errno 28] No space left on device'),\n",
            "('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/vectors',\n",
            "'/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/vectors',\n",
            "'[Errno 28] No space left on device'),\n",
            "('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/key2row',\n",
            "'/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/key2row',\n",
            "'[Errno 28] No space left on device'),\n",
            "('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/vectors.cfg',\n",
            "'/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/vectors.cfg',\n",
            "'[Errno 28] No space left on device'),\n",
            "('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/lookups.bin',\n",
            "'/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/lookups.bin',\n",
            "'[Errno 28] No space left on device')])\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 110, in train\n",
            "    save_checkpoint(is_best_checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 73, in save_checkpoint\n",
            "    shutil.copytree(output_path / DIR_MODEL_LAST, output_path / DIR_MODEL_BEST)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 368, in copytree\n",
            "    raise Error(errors)\n",
            "shutil.Error: [('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/strings.json', '/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/strings.json', '[Errno 28] No space left on device'), ('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/vectors', '/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/vectors', '[Errno 28] No space left on device'), ('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/key2row', '/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/key2row', '[Errno 28] No space left on device'), ('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/vectors.cfg', '/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/vectors.cfg', '[Errno 28] No space left on device'), ('/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-last/vocab/lookups.bin', '/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-10/model-best/vocab/lookups.bin', '[Errno 28] No space left on device')]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 126, in train\n",
            "    save_checkpoint(False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 67, in save_checkpoint\n",
            "    before_to_disk(nlp).to_disk(output_path / DIR_MODEL_LAST)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1985, in to_disk\n",
            "    util.to_disk(path, serializers, exclude)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 1287, in to_disk\n",
            "    writer(path / key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1974, in <lambda>\n",
            "    p, exclude=[\"vocab\"]\n",
            "  File \"spacy/tokenizer.pyx\", line 738, in spacy.tokenizer.Tokenizer.to_disk\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 04:51:35,887] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55df276a4000 @  0x7fea4ecf1001 0x7fea4c7ef54f 0x7fea4c83fb58 0x7fea4c843b17 0x7fea4c8e2203 0x55df1d29e544 0x55df1d29e240 0x55df1d312627 0x55df1d30cced 0x55df1d29fbda 0x55df1d30e737 0x55df1d30cced 0x55df1d29fbda 0x55df1d30e737 0x55df1d30c9ee 0x55df1d1dee2b 0x55df1d30efe4 0x55df1d30cced 0x55df1d1dee2b 0x55df1d30efe4 0x55df1d30c9ee 0x55df1d2a048c 0x55df1d2a0698 0x55df1d30efe4 0x55df1d29fafa 0x55df1d30dc0d 0x55df1d30cced 0x55df1d29fbda 0x55df1d30dc0d 0x55df1d30cced 0x55df1d29fbda\n",
            "2665029it [03:05, 14334.71it/s]\n",
            "[2022-01-06 04:54:43,490] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 05:13:50,558] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5580bc2b6000 @  0x7f658248b001 0x7f657ff8954f 0x7f657ffd9b58 0x7f657ffddb17 0x7f658007c203 0x5580b18da544 0x5580b18da240 0x5580b194e627 0x5580b1948ced 0x5580b18dbbda 0x5580b194a737 0x5580b1948ced 0x5580b18dbbda 0x5580b194a737 0x5580b19489ee 0x5580b181ae2b 0x5580b194afe4 0x5580b1948ced 0x5580b181ae2b 0x5580b194afe4 0x5580b19489ee 0x5580b18dc48c 0x5580b18dc698 0x5580b194afe4 0x5580b18dbafa 0x5580b1949c0d 0x5580b1948ced 0x5580b18dbbda 0x5580b1949c0d 0x5580b1948ced 0x5580b18dbbda\n",
            "2665029it [03:00, 14741.01it/s]\n",
            "[2022-01-06 05:16:52,959] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-4..6.wordngram-4.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 05:31:03,079] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x5637f7eaa000 @  0x7fd773163001 0x7fd770c6154f 0x7fd770cb1b58 0x7fd770cb5b17 0x7fd770d54203 0x5637eee0f544 0x5637eee0f240 0x5637eee83627 0x5637eee7dced 0x5637eee10bda 0x5637eee7f737 0x5637eee7dced 0x5637eee10bda 0x5637eee7f737 0x5637eee7d9ee 0x5637eed4fe2b 0x5637eee7ffe4 0x5637eee7dced 0x5637eed4fe2b 0x5637eee7ffe4 0x5637eee7d9ee 0x5637eee1148c 0x5637eee11698 0x5637eee7ffe4 0x5637eee10afa 0x5637eee7ec0d 0x5637eee7dced 0x5637eee10bda 0x5637eee7ec0d 0x5637eee7dced 0x5637eee10bda\n",
            "2665029it [03:02, 14580.61it/s]\n",
            "[2022-01-06 05:34:07,495] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 05:53:42,391] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55e7c4fc2000 @  0x7fecff224001 0x7fecfcd2254f 0x7fecfcd72b58 0x7fecfcd76b17 0x7fecfce15203 0x55e7ba9bc544 0x55e7ba9bc240 0x55e7baa30627 0x55e7baa2aced 0x55e7ba9bdbda 0x55e7baa2c737 0x55e7baa2aced 0x55e7ba9bdbda 0x55e7baa2c737 0x55e7baa2a9ee 0x55e7ba8fce2b 0x55e7baa2cfe4 0x55e7baa2aced 0x55e7ba8fce2b 0x55e7baa2cfe4 0x55e7baa2a9ee 0x55e7ba9be48c 0x55e7ba9be698 0x55e7baa2cfe4 0x55e7ba9bdafa 0x55e7baa2bc0d 0x55e7baa2aced 0x55e7ba9bdbda 0x55e7baa2bc0d 0x55e7baa2aced 0x55e7ba9bdbda\n",
            "2665029it [03:05, 14363.29it/s]\n",
            "[2022-01-06 05:56:49,593] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 06:16:36,401] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55c0664a4000 @  0x7f5d28b69001 0x7f5d2666754f 0x7f5d266b7b58 0x7f5d266bbb17 0x7f5d2675a203 0x55c05d234544 0x55c05d234240 0x55c05d2a8627 0x55c05d2a2ced 0x55c05d235bda 0x55c05d2a4737 0x55c05d2a2ced 0x55c05d235bda 0x55c05d2a4737 0x55c05d2a29ee 0x55c05d174e2b 0x55c05d2a4fe4 0x55c05d2a2ced 0x55c05d174e2b 0x55c05d2a4fe4 0x55c05d2a29ee 0x55c05d23648c 0x55c05d236698 0x55c05d2a4fe4 0x55c05d235afa 0x55c05d2a3c0d 0x55c05d2a2ced 0x55c05d235bda 0x55c05d2a3c0d 0x55c05d2a2ced 0x55c05d235bda\n",
            "2665029it [03:04, 14481.12it/s]\n",
            "[2022-01-06 06:19:42,057] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-15.subwords-2..5.wordngram-3.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-5.neg_sampling-10\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 06:35:21,400] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-5.neg_sampling-10.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55c80fd40000 @  0x7f22504cc001 0x7f224dfca54f 0x7f224e01ab58 0x7f224e01eb17 0x7f224e0bd203 0x55c805e94544 0x55c805e94240 0x55c805f08627 0x55c805f02ced 0x55c805e95bda 0x55c805f04737 0x55c805f02ced 0x55c805e95bda 0x55c805f04737 0x55c805f029ee 0x55c805dd4e2b 0x55c805f04fe4 0x55c805f02ced 0x55c805dd4e2b 0x55c805f04fe4 0x55c805f029ee 0x55c805e9648c 0x55c805e96698 0x55c805f04fe4 0x55c805e95afa 0x55c805f03c0d 0x55c805f02ced 0x55c805e95bda 0x55c805f03c0d 0x55c805f02ced 0x55c805e95bda\n",
            "2665029it [03:04, 14412.73it/s]\n",
            "[2022-01-06 06:38:27,922] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-5.neg_sampling-10.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-5.neg_sampling-10\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-15\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'uk'\u001b[0m\n",
            "[2022-01-06 06:59:13,307] [INFO] Reading vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-15.vec\n",
            "tcmalloc: large alloc 3198042112 bytes == 0x55b26b724000 @  0x7f15ec93b001 0x7f15ea43954f 0x7f15ea489b58 0x7f15ea48db17 0x7f15ea52c203 0x55b261fc9544 0x55b261fc9240 0x55b26203d627 0x55b262037ced 0x55b261fcabda 0x55b262039737 0x55b262037ced 0x55b261fcabda 0x55b262039737 0x55b2620379ee 0x55b261f09e2b 0x55b262039fe4 0x55b262037ced 0x55b261f09e2b 0x55b262039fe4 0x55b2620379ee 0x55b261fcb48c 0x55b261fcb698 0x55b262039fe4 0x55b261fcaafa 0x55b262038c0d 0x55b262037ced 0x55b261fcabda 0x55b262038c0d 0x55b262037ced 0x55b261fcabda\n",
            "2665029it [03:03, 14505.02it/s]\n",
            "[2022-01-06 07:02:18,664] [INFO] Loaded vectors from /tmp/FastText/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-15.vec\n",
            "\u001b[38;5;2m✔ Successfully converted 2665029 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/tmp/FastText\n",
            "\u001b[38;5;2m✔ Created output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-2..6.wordngram-5.neg_sampling-15\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 69, in train\n",
            "    config = util.load_config(config_path, overrides=overrides, interpolate=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/util.py\", line 648, in load_config\n",
            "    config_path, overrides=overrides, interpolate=interpolate\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/config.py\", line 455, in from_disk\n",
            "    text = file_.read()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Converting vectors from ubertext.fiction_news_wikipedia.filter_rus+short.tokens.txt.algo-skipgram.epochs-10.subwords-5..6.wordngram-4.neg_sampling-10\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "for f in glob(\"/gdrive/MyDrive/UberGrid/vectors/*\"):\n",
        "    bf, _ = os.path.splitext(os.path.basename(f))\n",
        "\n",
        "    if os.path.exists(f\"/gdrive/MyDrive/SpaCyResults/{bf}\"):\n",
        "        print(f\"Skipping {bf} as their already exists\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Converting vectors from {bf}\")\n",
        "\n",
        "    !rm -rf /tmp/FastText\n",
        "    !mkdir /tmp/FastText\n",
        "    txt_vectors = os.path.join(\"/tmp/FastText/\", bf + \".vec\")\n",
        "\n",
        "    convert_bin_to_vec(f, txt_vectors)\n",
        "    !python -m spacy init vectors uk {txt_vectors} /tmp/FastText\n",
        "\n",
        "    !python -m spacy train /gdrive/MyDrive/SpaCyResults/ner_uk_cpu_init.config --output /gdrive/MyDrive/SpaCyResults/{bf} --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC42LDhlS-KU",
        "outputId": "bdd290da-40af-4759-dec2-51338b5bc865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ubercorpus.cased.tokenized.fasttext\": {\n",
            "        \"ents_f\": 0.7942051994,\n",
            "        \"ents_p\": 0.813745425,\n",
            "        \"ents_per_type\": {\n",
            "            \"LOC\": {\n",
            "                \"f\": 0.7868852459,\n",
            "                \"p\": 0.7959183673,\n",
            "                \"r\": 0.7780548628\n",
            "            },\n",
            "            \"MISC\": {\n",
            "                \"f\": 0.4009661836,\n",
            "                \"p\": 0.4770114943,\n",
            "                \"r\": 0.3458333333\n",
            "            },\n",
            "            \"ORG\": {\n",
            "                \"f\": 0.4945295405,\n",
            "                \"p\": 0.5765306122,\n",
            "                \"r\": 0.4329501916\n",
            "            },\n",
            "            \"PERS\": {\n",
            "                \"f\": 0.8847407407,\n",
            "                \"p\": 0.8797878609,\n",
            "                \"r\": 0.889749702\n",
            "            }\n",
            "        },\n",
            "        \"ents_r\": 0.7755813953,\n",
            "        \"ner_loss\": 449.4310678729,\n",
            "        \"tok2vec_loss\": 108.8518468687\n",
            "    },\n",
            "    \"ubercorpus1.0.skipgram.dim300.epoch10.subw2-5.neg10.wordngram4\": {\n",
            "        \"ents_f\": 0.8140544809,\n",
            "        \"ents_p\": 0.8294448914,\n",
            "        \"ents_per_type\": {\n",
            "            \"LOC\": {\n",
            "                \"f\": 0.7881040892,\n",
            "                \"p\": 0.7832512315,\n",
            "                \"r\": 0.7930174564\n",
            "            },\n",
            "            \"MISC\": {\n",
            "                \"f\": 0.5042735043,\n",
            "                \"p\": 0.5175438596,\n",
            "                \"r\": 0.4916666667\n",
            "            },\n",
            "            \"ORG\": {\n",
            "                \"f\": 0.4953703704,\n",
            "                \"p\": 0.6257309942,\n",
            "                \"r\": 0.4099616858\n",
            "            },\n",
            "            \"PERS\": {\n",
            "                \"f\": 0.904435844,\n",
            "                \"p\": 0.9036287924,\n",
            "                \"r\": 0.9052443385\n",
            "            }\n",
            "        },\n",
            "        \"ents_r\": 0.7992248062,\n",
            "        \"ner_loss\": 175.8744733015,\n",
            "        \"tok2vec_loss\": 171.8070757954\n",
            "    },\n",
            "    \"ubercorpus1.0_combined_with_lemmas.skipgram.dim300.epoch10.subw2-5.neg10.wordngram4\": {\n",
            "        \"ents_f\": 0.8027695351,\n",
            "        \"ents_p\": 0.8197979798,\n",
            "        \"ents_per_type\": {\n",
            "            \"LOC\": {\n",
            "                \"f\": 0.7854356307,\n",
            "                \"p\": 0.8206521739,\n",
            "                \"r\": 0.753117207\n",
            "            },\n",
            "            \"MISC\": {\n",
            "                \"f\": 0.4750499002,\n",
            "                \"p\": 0.4559386973,\n",
            "                \"r\": 0.4958333333\n",
            "            },\n",
            "            \"ORG\": {\n",
            "                \"f\": 0.4484304933,\n",
            "                \"p\": 0.5405405405,\n",
            "                \"r\": 0.3831417625\n",
            "            },\n",
            "            \"PERS\": {\n",
            "                \"f\": 0.9032644504,\n",
            "                \"p\": 0.9078868152,\n",
            "                \"r\": 0.8986889154\n",
            "            }\n",
            "        },\n",
            "        \"ents_r\": 0.7864341085,\n",
            "        \"ner_loss\": 105.696753009,\n",
            "        \"tok2vec_loss\": 122.6537018834\n",
            "    },\n",
            "    \"ubertext.fiction_news_wikipedia.filter_rus+short.tokens.skipgram.dim300.epoch10.subw2-5.neg10.lowercased.wordngram4\": {\n",
            "        \"ents_f\": 0.6771557009,\n",
            "        \"ents_p\": 0.7160760588,\n",
            "        \"ents_per_type\": {\n",
            "            \"LOC\": {\n",
            "                \"f\": 0.6110397946,\n",
            "                \"p\": 0.6296296296,\n",
            "                \"r\": 0.5935162095\n",
            "            },\n",
            "            \"MISC\": {\n",
            "                \"f\": 0.3597430407,\n",
            "                \"p\": 0.3700440529,\n",
            "                \"r\": 0.35\n",
            "            },\n",
            "            \"ORG\": {\n",
            "                \"f\": 0.3705357143,\n",
            "                \"p\": 0.4438502674,\n",
            "                \"r\": 0.3180076628\n",
            "            },\n",
            "            \"PERS\": {\n",
            "                \"f\": 0.7825,\n",
            "                \"p\": 0.8226018397,\n",
            "                \"r\": 0.7461263409\n",
            "            }\n",
            "        },\n",
            "        \"ents_r\": 0.642248062,\n",
            "        \"ner_loss\": 67.6259003418,\n",
            "        \"tok2vec_loss\": 144.6307802337\n",
            "    },\n",
            "    \"ubertext.fiction_news_wikipedia.filter_rus+short.tokens.skipgram.dim300.epoch10.subw2-6.neg10.wordngram3.vec\": {\n",
            "        \"ents_f\": 0.8152992469,\n",
            "        \"ents_p\": 0.8341443633,\n",
            "        \"ents_per_type\": {\n",
            "            \"LOC\": {\n",
            "                \"f\": 0.8037825059,\n",
            "                \"p\": 0.7640449438,\n",
            "                \"r\": 0.8478802993\n",
            "            },\n",
            "            \"MISC\": {\n",
            "                \"f\": 0.4705882353,\n",
            "                \"p\": 0.5405405405,\n",
            "                \"r\": 0.4166666667\n",
            "            },\n",
            "            \"ORG\": {\n",
            "                \"f\": 0.5033707865,\n",
            "                \"p\": 0.6086956522,\n",
            "                \"r\": 0.4291187739\n",
            "            },\n",
            "            \"PERS\": {\n",
            "                \"f\": 0.9039039039,\n",
            "                \"p\": 0.9110169492,\n",
            "                \"r\": 0.8969010727\n",
            "            }\n",
            "        },\n",
            "        \"ents_r\": 0.7972868217,\n",
            "        \"ner_loss\": 163.2132899196,\n",
            "        \"tok2vec_loss\": 102.557713485\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os.path\n",
        "from glob import glob\n",
        "import json\n",
        "results = {}\n",
        "\n",
        "results_path = Path(\"/gdrive/MyDrive/SpaCyResults/\")\n",
        "for f in glob(\"/gdrive/MyDrive/WordVectors/FastText/textual/*\"):\n",
        "    bf, _ = os.path.splitext(os.path.basename(f))\n",
        "    meta_file = results_path / bf / \"model-best/meta.json\"\n",
        "    if not os.path.exists(meta_file):\n",
        "        continue\n",
        "    with open(meta_file, \"r\") as fp:\n",
        "        meta = json.load(fp)\n",
        "        results[bf] = meta[\"performance\"]\n",
        "\n",
        "print(json.dumps(results, indent=4, sort_keys=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6ICobUyaTdJ",
        "outputId": "3fd1961f-26e8-402c-d601-f787e0df8c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"lang\":\"uk\",\n",
            "  \"name\":\"pipeline\",\n",
            "  \"version\":\"0.0.0\",\n",
            "  \"spacy_version\":\">=3.1.4,<3.2.0\",\n",
            "  \"description\":\"\",\n",
            "  \"author\":\"\",\n",
            "  \"email\":\"\",\n",
            "  \"url\":\"\",\n",
            "  \"license\":\"\",\n",
            "  \"spacy_git_version\":\"006df1ae1\",\n",
            "  \"vectors\":{\n",
            "    \"width\":300,\n",
            "    \"vectors\":595119,\n",
            "    \"keys\":595119,\n",
            "    \"name\":\"uk_pipeline.vectors\"\n",
            "  },\n",
            "  \"labels\":{\n",
            "    \"tok2vec\":[\n",
            "\n",
            "    ],\n",
            "    \"ner\":[\n",
            "      \"LOC\",\n",
            "      \"MISC\",\n",
            "      \"ORG\",\n",
            "      \"PERS\"\n",
            "    ]\n",
            "  },\n",
            "  \"pipeline\":[\n",
            "    \"tok2vec\",\n",
            "    \"ner\"\n",
            "  ],\n",
            "  \"components\":[\n",
            "    \"tok2vec\",\n",
            "    \"ner\"\n",
            "  ],\n",
            "  \"disabled\":[\n",
            "\n",
            "  ],\n",
            "  \"performance\":{\n",
            "    \"ents_f\":0.7954363491,\n",
            "    \"ents_p\":0.8224337748,\n",
            "    \"ents_r\":0.7701550388,\n",
            "    \"ents_per_type\":{\n",
            "      \"PERS\":{\n",
            "        \"p\":0.888424821,\n",
            "        \"r\":0.8873659118,\n",
            "        \"f\":0.8878950507\n",
            "      },\n",
            "      \"ORG\":{\n",
            "        \"p\":0.6321243523,\n",
            "        \"r\":0.4674329502,\n",
            "        \"f\":0.5374449339\n",
            "      },\n",
            "      \"LOC\":{\n",
            "        \"p\":0.7742782152,\n",
            "        \"r\":0.7356608479,\n",
            "        \"f\":0.7544757033\n",
            "      },\n",
            "      \"MISC\":{\n",
            "        \"p\":0.4879518072,\n",
            "        \"r\":0.3375,\n",
            "        \"f\":0.3990147783\n",
            "      }\n",
            "    },\n",
            "    \"tok2vec_loss\":107.6293297585,\n",
            "    \"ner_loss\":62.8648724961\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "!cat \"/gdrive/MyDrive/SpaCyResults/ubertext.fiction_news_wikipedia.filter_rus+short.tokens.skipgram.dim300.epoch10.subw2-5.neg10.lowercased.wordngram4/model-best/meta.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntTIc7taTyRi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Extrinsic NER: Spacy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPpaUim8GvV9eTFlK2AAyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}